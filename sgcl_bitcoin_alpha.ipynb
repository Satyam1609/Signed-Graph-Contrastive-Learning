{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VySJAWTwMAjF",
        "outputId": "39be29ab-3efe-4afc-e91b-4d3bde8cc222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d__jTszQPdTk"
      },
      "outputs": [],
      "source": [
        "# !pip install dill\n",
        "!pip install dotmap\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu117/repo.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgO5hiEZPt86"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "APDnKo-G_WHp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "EKAdF8lBL5Nd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# import dill\n",
        "from dotmap import DotMap\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# pd.set_option('display.max_rows', 100)\n",
        "# pd.set_option('display.min_rows', 100)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "import dgl\n",
        "import dgl.nn\n",
        "import dgl.function as fn\n",
        "from dgl.nn import RelGraphConv\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "\n",
        "# from my_nn import GraphConv, GATConv, HeteroGraphConv\n",
        "import pickle\n",
        "import tqdm as tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "0YUN0otFNcwg"
      },
      "outputs": [],
      "source": [
        "seed = 100\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "pPKT13z0rftX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ejdNo8DrxtJ"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iyce70RXwMzn",
        "outputId": "6d2ffc26-7889-4393-9aed-7fd562c1fe9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ce76b076-bb6a-4ac1-853f-43a0a665a034\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>dst</th>\n",
              "      <th>label</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>430</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1376539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3134</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1369713600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3026</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1350014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3010</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1347854400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>804</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1337572800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce76b076-bb6a-4ac1-853f-43a0a665a034')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce76b076-bb6a-4ac1-853f-43a0a665a034 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce76b076-bb6a-4ac1-853f-43a0a665a034');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    src  dst  label           3\n",
              "1   430    1     10  1376539200\n",
              "2  3134    1     10  1369713600\n",
              "3  3026    1     10  1350014400\n",
              "4  3010    1     10  1347854400\n",
              "5   804    1     10  1337572800"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/SGCL_NEW/newe_datasets/bitcoin_alpha/soc-sign-bitcoinalpha.csv',header=None)\n",
        "# df.head()\n",
        "df=df.rename(columns={0: \"src\", 1: \"dst\",2:\"label\"})\n",
        "# df=df.drop(columns=0)\n",
        "df=df[1:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chJahh6Qx5Lo"
      },
      "source": [
        "# Load user and features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-xTb_w0Sd7f",
        "outputId": "0c54bee2-c350-4640-8c86-3e5e23ff433e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3783, 64)\n"
          ]
        }
      ],
      "source": [
        "############################################### Load user and features ############################################################\n",
        "het_num_nodes_dict = {}\n",
        "het_node_feat_dict = {}\n",
        "het_data_dict = {}\n",
        "het_edge_feat_dict = {}\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/bitcoin_alpha/user_dict.pkl\", \"rb\") as f:\n",
        "    dict_user2id = pickle.load(f)\n",
        "het_num_nodes_dict['user'] = len(dict_user2id)\n",
        "\n",
        "# if dataset_name == 'BitCoinAlpha' or dataset_name == 'BitCoinOTC':\n",
        "user_feat = np.random.rand(len(dict_user2id), 64)\n",
        "het_node_feat_dict['user'] = user_feat\n",
        "    \n",
        "print(het_node_feat_dict['user'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJe5dKvJXQMd",
        "outputId": "a217ced8-799f-40fe-82b9-6c041d2f2c71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3783"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dict_user2id.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5ovSBsMsTAai"
      },
      "outputs": [],
      "source": [
        "# dict_user2id.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fEh1Ma7xyfly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUIyIKo8yiit"
      },
      "source": [
        "# Load Train Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "tUbAKBdZSfyR"
      },
      "outputs": [],
      "source": [
        "############################################### Load Train Graph ############################################################\n",
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/bitcoin_alpha/g_train.pkl\", \"rb\") as f:\n",
        "    tmp_het_data_dict = pickle.load(f)\n",
        "    tmp_het_edata_dict = tmp_het_data_dict\n",
        "het_data_dict.update(tmp_het_data_dict)\n",
        "\n",
        "# for k in tmp_het_edata_dict:\n",
        "#     het_edge_feat_dict[k] = torch.from_numpy(tmp_het_edata_dict[k])\n",
        "\n",
        "graph_user = dgl.heterograph(\n",
        "    data_dict = het_data_dict,\n",
        "    num_nodes_dict = het_num_nodes_dict\n",
        ")\n",
        "for node_t in het_node_feat_dict:\n",
        "    graph_user.nodes[node_t].data['feature'] = torch.from_numpy(het_node_feat_dict[node_t]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHaVGdA8SofK",
        "outputId": "84c20920-0cb6-4d93-9013-942a645b09aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "items\n",
            "(('user', 'positive', 'user'), (array([   0,    1,    2, ..., 1061, 1064, 1060]), array([ 398,  398,  398, ..., 1064, 1060, 1064])))\n",
            "items\n",
            "(('user', 'negative', 'user'), (array([ 398,  398,  398, ...,  614, 1063, 1064]), array([3360, 3361, 3362, ..., 1064,  132,  132])))\n"
          ]
        }
      ],
      "source": [
        "for i in tmp_het_data_dict.items():\n",
        "  print('items')\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8088Dnzy5m5"
      },
      "source": [
        "# Load Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "QR77uyVINf-e"
      },
      "outputs": [],
      "source": [
        "############################################### Load Labels ############################################################\n",
        "class LabelPairs(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(LabelPairs).__init__()\n",
        "        u = torch.from_numpy(df.src.values).long()\n",
        "        v = torch.from_numpy(df.dst.values).long()\n",
        "        y = torch.from_numpy(df['label'].values).double()\n",
        "        self.pairs = torch.stack((u, v), dim=0)\n",
        "        self.label = y\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.pairs[:, index], self.label[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "class NodeBatch(torch.utils.data.Dataset):\n",
        "    def __init__(self, nodes):\n",
        "        self.nodes = torch.from_numpy(nodes)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.nodes[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zssRuFezy-lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzYJ7rXzBas"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "9Trrxx9V_nvR"
      },
      "outputs": [],
      "source": [
        "\"\"\"Torch modules for graph convolutions(GCN).\"\"\"\n",
        "# pylint: disable= no-member, arguments-differ, invalid-name\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.functional import F\n",
        "\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "from dgl.nn import utils\n",
        "\n",
        "'''\n",
        "# The GraphConv class is defined as a subclass of nn.Module. It takes several parameters as inputs:\n",
        "\n",
        "# in_feats: The number of input features per node.\n",
        "# out_feats: The number of output features per node.\n",
        "# norm: A string indicating the type of normalization to use. It can take one of the following values: \"none\", \"both\", or \"right\".\n",
        "# weight: A boolean indicating whether to use a learnable weight matrix.\n",
        "# bias: A boolean indicating whether to use a learnable bias vector.\n",
        "# activation: An activation function to apply to the output tensor. If None, no activation is applied.\n",
        "# residual: A boolean indicating whether to use residual connections.\n",
        "# allow_zero_in_degree: A boolean indicating whether to allow nodes with zero in-degree in the graph.\n",
        "# The __init__ method initializes the parameters of the GraphConv module, including the weight and bias parameters, and sets the activation function and normalization type.\n",
        "\n",
        "# The reset_parameters method initializes the weight and bias parameters of the module using the Xavier initialization and zeros initialization, respectively.\n",
        "\n",
        "# The set_allow_zero_in_degree method sets the allow_zero_in_degree parameter of the module.\n",
        "\n",
        "# The forward method implements the forward pass of the GraphConv module. It takes as input a graph (graph), a feature tensor (feat), and an optional weight matrix (weight). It applies the graph convolution operation to the feature tensor and returns the output tensor.\n",
        "\n",
        "# Inside the forward method, the graph.local_scope() context manager is used to create a local scope for any changes made to the graph.\n",
        "\n",
        "# If allow_zero_in_degree is set to False, the method checks whether the graph contains nodes with zero in-degree. If so, it raises a DGLError.\n",
        "\n",
        "# If the norm parameter is set to \"both\", it normalizes the feature tensor using the degree of the outgoing edges of each node in the graph.\n",
        "\n",
        "# If residual is set to True, it adds a self-loop to the graph and applies a linear transformation to the feature tensor to compute a residual connection.\n",
        "\n",
        "# If weight is not provided, it uses the weight parameter of the module. If in_feats is greater than out_feats, it applies the weight matrix to the feature tensor before aggregating the neighbors' features. Otherwise, it aggregates the neighbors' features before applying the weight matrix.\n",
        "\n",
        "# If the norm parameter is set to \"none\", it normalizes the output tensor by dividing it by the in-degree of each node in the graph. If it is set to \"both\", it multiplies the output tensor by the square root of the inverse of the in-degree and the out-degree of each node in the graph.\n",
        "\n",
        "# If bias is set to True, it adds the bias vector to the output tensor.\n",
        "\n",
        "# If activation is provided, it applies the activation function to the output tensor.\n",
        "\n",
        "# If residual is set to True, it adds the residual connection to the output tensor.\n",
        "\n",
        "# Finally, it returns the output tensor. '''\n",
        "class GraphConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 norm='right',\n",
        "                 weight=True,\n",
        "                 bias=True,\n",
        "                 activation=None,\n",
        "                 residual=True,\n",
        "                 allow_zero_in_degree=False):\n",
        "        super(GraphConv, self).__init__()\n",
        "#         if norm not in ('none', 'both', 'right'):\n",
        "        if norm not in ('right'):\n",
        "            raise DGLError('Invalid norm value. Must be either \"none\", \"both\" or \"right\".'\n",
        "                           ' But got \"{}\".'.format(norm))\n",
        "        self._in_feats = in_feats\n",
        "        self._out_feats = out_feats\n",
        "        self._norm = norm\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        self._residual = residual\n",
        "        \n",
        "        if self._residual:\n",
        "            self.loop_weight = nn.Linear(in_feats, out_feats, bias=False)\n",
        "        \n",
        "        if weight:\n",
        "            self.weight = nn.Parameter(th.Tensor(in_feats, out_feats))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(th.Tensor(out_feats))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self._activation = activation\n",
        "\n",
        "    def reset_parameters(self):\n",
        "\n",
        "        if self.weight is not None:\n",
        "            init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, weight=None):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            if self._norm == 'both':\n",
        "                degs = graph.out_degrees().float().clamp(min=1)\n",
        "                norm = th.pow(degs, -0.5)\n",
        "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                feat_src = feat_src * norm\n",
        "\n",
        "            if self._residual:\n",
        "                loop_message = self.loop_weight(feat_dst)\n",
        "            if graph.num_edges() == 0:\n",
        "                return loop_message                \n",
        "                \n",
        "            if weight is not None:\n",
        "                if self.weight is not None:\n",
        "                    raise DGLError('External weight is provided while at the same time the'\n",
        "                                   ' module has defined its own weight parameter. Please'\n",
        "                                   ' create the module with flag weight=False.')\n",
        "            else:\n",
        "                weight = self.weight\n",
        "                \n",
        "            if self._in_feats > self._out_feats:\n",
        "                # mult W first to reduce the feature size for aggregation.\n",
        "                if weight is not None:\n",
        "                    feat_src = th.matmul(feat_src, weight)\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                 fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "            else:\n",
        "                # aggregate first then mult W\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                 fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "                if weight is not None:\n",
        "                    rst = th.matmul(rst, weight)\n",
        "\n",
        "            if self._norm != 'none':\n",
        "                degs = graph.in_degrees().float().clamp(min=1)\n",
        "                if self._norm == 'both':\n",
        "                    norm = th.pow(degs, -0.5)\n",
        "                else:\n",
        "                    norm = 1.0 / degs\n",
        "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                rst = rst * norm\n",
        "\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias\n",
        "\n",
        "            if self._activation is not None:\n",
        "                rst = self._activation(rst)\n",
        "                \n",
        "            if self._residual:\n",
        "                rst = rst + loop_message\n",
        "\n",
        "            return rst\n",
        "\n",
        "  '''\n",
        " The GATConv module takes as input a graph and node feature tensor and produces an output feature tensor of the same shape. The constructor for the module takes the following arguments:\n",
        "\n",
        "in_feats: the number of input node features.\n",
        "out_feats: the number of output node features.\n",
        "num_heads: the number of attention heads to use.\n",
        "feat_drop: the probability of dropping out input node features during training.\n",
        "attn_drop: the probability of dropping out attention weights during training.\n",
        "negative_slope: the slope of the leaky ReLU activation function used in computing the attention coefficients.\n",
        "residual: whether to include a residual connection in the module.\n",
        "activation: the activation function to use in the module.\n",
        "allow_zero_in_degree: whether to allow nodes in the input graph with zero in-degree.\n",
        "The forward method of the module takes a graph and node feature tensor as input and produces an output feature tensor by performing the following steps:\n",
        "\n",
        "Apply dropout to the input node features.\n",
        "Compute the linear projections of the input node features for each attention head.\n",
        "Compute the attention coefficients for each edge in the graph using the dot product of the linear projections of the source and destination nodes for each attention head.\n",
        "Apply the attention coefficients to the source node features to compute the message passed to the destination nodes for each attention head.\n",
        "Concatenate the messages from all attention heads and sum them to produce the output node features.\n",
        "Apply activation and/or residual connection if specified.\n",
        "Overall, this code provides an implementation of the GATConv module that can be used in building GAT-based GNNs for graph classification, node classification, and other graph-related tasks. \n",
        "\n",
        "'''    \n",
        "    \n",
        "class GATConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False):\n",
        "        super(GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            if isinstance(feat, tuple):\n",
        "                h_src = self.feat_drop(feat[0])\n",
        "                h_dst = self.feat_drop(feat[1])\n",
        "                if not hasattr(self, 'fc_src'):\n",
        "                    self.fc_src, self.fc_dst = self.fc, self.fc\n",
        "                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "            else:\n",
        "                h_src = h_dst = self.feat_drop(feat)\n",
        "                feat_src = feat_dst = self.fc(h_src).view(\n",
        "                    -1, self._num_heads, self._out_feats)\n",
        "                if graph.is_block:\n",
        "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                    h_dst = h_src[:graph.number_of_dst_nodes()]\n",
        "            # NOTE: GAT paper uses \"first concatenation then linear projection\"\n",
        "            # to compute attention scores, while ours is \"first projection then\n",
        "            # addition\", the two approaches are mathematically equivalent:\n",
        "            # We decompose the weight vector a mentioned in the paper into\n",
        "            # [a_l || a_r], then\n",
        "            # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j\n",
        "            # Our implementation is much efficient because we do not need to\n",
        "            # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,\n",
        "            # addition could be optimized with DGL's built-in function u_add_v,\n",
        "            # which further speeds up computation and saves memory footprint.\n",
        "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
        "            graph.dstdata.update({'er': er})\n",
        "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
        "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "            e = self.leaky_relu(graph.edata.pop('e'))\n",
        "            # compute softmax\n",
        "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "            # message passing\n",
        "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                             fn.sum('m', 'ft'))\n",
        "            rst = graph.dstdata['ft']\n",
        "            # residual\n",
        "            if self.res_fc is not None:\n",
        "                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n",
        "                rst = rst + resval\n",
        "            # activation\n",
        "            if self.activation:\n",
        "                rst = self.activation(rst)\n",
        "                \n",
        "            rst = rst.view(len(rst), -1)\n",
        "                \n",
        "            return rst\n",
        "    \n",
        "\n",
        "    \n",
        "''' \n",
        "The HeteroGraphConv class takes several inputs: mods, a dictionary of modules for each edge type; dim_values, the dimension of the input node embeddings; dim_query, the dimension of the attention query; and agg_type, the type of aggregation function to use during message passing.\n",
        "\n",
        "The constructor initializes the modules for each edge type in mods, and sets the input dimensions and attention layers for each module. \n",
        "If the agg_type is specified, the corresponding aggregation function is also set.\n",
        "\n",
        "The forward method takes as input a graph g, a set of node embeddings inputs, and optional arguments for the edge type modules. \n",
        "The method first checks whether the input is a tuple or a block diagonal graph, and assigns the source and destination node embeddings accordingly. \n",
        "It then loops through each canonical edge type in the graph, and computes the output embeddings and attention scores for each type. \n",
        "If an edge type has no edges, the output and attention scores are set to zero.\n",
        "\n",
        "After processing all edge types, the method aggregates the output embeddings based on the agg_type specified during initialization. \n",
        "If the agg_type is attention-based, the attention scores are used to weight the output embeddings. Finally, the method returns the aggregated output embeddings and the attention scores.\n",
        "''' \n",
        "    \n",
        "class HeteroGraphConv(nn.Module):\n",
        "    def __init__(self, mods, dim_values, dim_query, agg_type='attn'):\n",
        "        super(HeteroGraphConv, self).__init__()\n",
        "        self.mods = nn.ModuleDict(mods)\n",
        "        # Do not break if graph has 0-in-degree nodes.\n",
        "        # Because there is no general rule to add self-loop for heterograph.\n",
        "        for _, v in self.mods.items():\n",
        "            set_allow_zero_in_degree_fn = getattr(v, 'set_allow_zero_in_degree', None)\n",
        "            if callable(set_allow_zero_in_degree_fn):\n",
        "                set_allow_zero_in_degree_fn(True)\n",
        "                \n",
        "        self.dim_values = dim_values\n",
        "        self.dim_query = dim_query\n",
        "        \n",
        "        self.attention = nn.ModuleDict()\n",
        "        for k, _ in self.mods.items():\n",
        "            self.attention[k] = nn.Sequential(nn.Linear(dim_values, dim_query), nn.Tanh(), nn.Linear(dim_query, 1, bias=False))\n",
        "        \n",
        "        self.agg_type = agg_type\n",
        "        if agg_type == 'sum':\n",
        "            self.agg_fn = th.sum\n",
        "        elif agg_type == 'max':\n",
        "            self.agg_fn = lambda inputs, dim: th.max(inputs, dim=dim)[0]\n",
        "        elif agg_type == 'min':\n",
        "            self.agg_fn = lambda inputs, dim: th.min(inputs, dim=dim)[0]\n",
        "        elif agg_type == 'stack':\n",
        "            self.agg_fn = th.stack\n",
        "        \n",
        "    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
        "        if mod_args is None:\n",
        "            mod_args = {}\n",
        "        if mod_kwargs is None:\n",
        "            mod_kwargs = {}\n",
        "        outputs = []\n",
        "        et_scores = []\n",
        "        et_count = 0\n",
        "        if isinstance(inputs, tuple) or g.is_block:\n",
        "            if isinstance(inputs, tuple):\n",
        "                src_inputs, dst_inputs = inputs\n",
        "            else:\n",
        "                src_inputs = inputs\n",
        "                dst_inputs = inputs[:g.number_of_dst_nodes()]\n",
        "\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    et_scores.append(torch.zeros(g.number_of_dst_nodes(), 1).to(g.device))\n",
        "                    outputs.append(torch.zeros(g.number_of_dst_nodes(), self.dim_values).to(g.device))\n",
        "                    continue\n",
        "                et_count += 1\n",
        "                dstdata = self.mods[etype](rel_graph, (src_inputs, dst_inputs))\n",
        "                outputs.append(dstdata)\n",
        "                et_scores.append(self.attention[etype](dstdata))\n",
        "        if len(outputs) == 0:\n",
        "            out_embs = dst_inputs\n",
        "        else:\n",
        "            et_dst_data = torch.stack(outputs, dim=0)\n",
        "            if self.agg_type == 'attn':\n",
        "                attn = torch.softmax(torch.stack(et_scores, dim=0), dim=0)\n",
        "                out_embs = (attn * et_dst_data).sum(dim=0)\n",
        "            elif self.agg_type == 'attn_sum':\n",
        "                attn = torch.softmax(torch.stack(et_scores, dim=0).mean(dim=1, keepdims=True), dim=0)\n",
        "                out_embs = (attn * et_dst_data).sum(dim=0)\n",
        "            elif self.agg_type == 'mean':\n",
        "                out_embs = torch.sum(torch.stack(et_scores, dim=0), dim=0) / et_count\n",
        "            else:\n",
        "                out_embs = self.agg_fn(et_dst_data, dim=0)\n",
        "        return out_embs, attn    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "a0bYq22lNwVS"
      },
      "outputs": [],
      "source": [
        "############################################## Define Model ############################################################\n",
        "\"\"\"Torch modules for graph convolutions(GCN).\"\"\"\n",
        "# pylint: disable= no-member, arguments-differ, invalid-name\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.functional import F\n",
        "\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "from dgl.nn import utils\n",
        "\n",
        "\n",
        "'''\n",
        "Define Hetero Layer\n",
        "'''\n",
        "'''\n",
        "This is a PyTorch implementation of a heterogeneous graph neural network called HetAttn. \n",
        "The code contains two functions: calc_from_loader and forward. \n",
        "The calc_from_loader function takes in a data loader, node features, and a device and returns the node embeddings and attention results. \n",
        "The forward function takes in a graph, node features, node IDs, and a device and returns the node embeddings and attention results for the specified nodes.\n",
        "'''\n",
        "\n",
        "'''\n",
        "The HetAttn class is defined as a subclass of nn.Module and takes in two arguments: args and etypes. The args argument is a dictionary of hyperparameters and the etypes argument is a list of edge types in the graph.\n",
        "\n",
        "The __init__ function initializes several instance variables.\n",
        "The args argument is converted to a DotMap object and saved as an instance variable.\n",
        "The etypes argument is saved as an instance variable. \n",
        "A linear layer called feature_trans is defined that takes in node features of size args.dim_features and outputs node embeddings of size args.dim_hiddens. \n",
        "A nn.ModuleList called convs is initialized to hold the graph convolutional layers that will be added later.\n",
        " \n",
        "'''\n",
        "'''\n",
        "This is a PyTorch module named HetAttn that implements a heterogeneous graph neural network (GNN) with attention mechanism. \n",
        "The __init__ method initializes the model's parameters, including linear layers and graph convolutional layers (GraphConv or GATConv). \n",
        "The calc_from_loader method calculates the embeddings for a batch of nodes, and the forward method applies the model to a graph and returns the embeddings for the specified nodes. \n",
        "The inference method is similar to the forward method, but uses a different sampler for better inference performance.\n",
        "\n",
        "The HetAttn class takes two arguments in its constructor: args and etypes. args is a dictionary-like object containing various hyperparameters of the model, while etypes is a list of the edge types in the heterogeneous graph.\n",
        "\n",
        "The feature_trans linear layer maps the input features to a higher-dimensional space. \n",
        "The convs list contains multiple HeteroGraphConv layers, each of which applies a graph convolution operation to the input. \n",
        "The specific type of graph convolution used depends on the value of args.conv_type. \n",
        "If args.conv_type is 'gcn', GraphConv is used, while if it is 'gat', GATConv is used. \n",
        "The number of graph convolution layers is controlled by args.conv_depth. \n",
        "The output of each graph convolution layer is concatenated with the previous layers' outputs and passed through another linear layer (concat_weight) to obtain the final node embeddings.\n",
        "\n",
        "The calc_from_loader method takes a loader object, which generates node pairs and their corresponding subgraphs. \n",
        "It iteratively applies the HeteroGraphConv layers to the input features and concatenates the resulting embeddings to obtain the final node embeddings.\n",
        "\n",
        "The forward and inference methods are similar, except for the type of neighbor sampler used. \n",
        "They both take a graph g, a feature matrix x, a list of node IDs nids, and a device on which to perform computations. \n",
        "They return the node embeddings for the specified nodes nids and the attention weights used during computation.\n",
        "'''\n",
        "class HetAttn(nn.Module):\n",
        "    def __init__(self, args, etypes):\n",
        "        super(HetAttn, self).__init__()\n",
        "        self.args = DotMap(args.toDict())\n",
        "        args = self.args\n",
        "        self.etypes = etypes\n",
        "        self.feature_trans = nn.Linear(args.dim_features, args.dim_hiddens, bias=False)\n",
        "        self.convs = nn.ModuleList()\n",
        "        \n",
        "        if self.args.conv_type == 'gcn':\n",
        "            for _ in range(args.conv_depth - 1):\n",
        "                self.convs.append(HeteroGraphConv({rel: GraphConv(args.dim_hiddens, args.dim_hiddens, allow_zero_in_degree=True, residual=args.residual)\n",
        "                                                   for rel in self.etypes},\n",
        "                                  agg_type=args.het_agg_type, dim_values=args.dim_hiddens, dim_query=args.dim_query)) \n",
        "            self.convs.append(HeteroGraphConv({rel: GraphConv(args.dim_hiddens, args.dim_embs, allow_zero_in_degree=True, residual=args.residual) \n",
        "                                               for rel in self.etypes},\n",
        "                              agg_type=args.het_agg_type, dim_values=args.dim_embs, dim_query=args.dim_query))\n",
        "        elif self.args.conv_type == 'gat':\n",
        "            for _ in range(args.conv_depth - 1):\n",
        "                self.convs.append(HeteroGraphConv({rel: GATConv(args.dim_hiddens, args.dim_hiddens // args.num_heads, args.num_heads, allow_zero_in_degree=True, residual=args.residual)\n",
        "                                                   for rel in self.etypes},\n",
        "                                  agg_type=args.het_agg_type, dim_values=args.dim_hiddens, dim_query=args.dim_query)) \n",
        "            self.convs.append(HeteroGraphConv({rel: GATConv(args.dim_hiddens, args.dim_embs // args.num_heads, args.num_heads, allow_zero_in_degree=True, residual=args.residual) \n",
        "                                               for rel in self.etypes},\n",
        "                          agg_type=args.het_agg_type, dim_values=args.dim_embs, dim_query=args.dim_query))\n",
        "        \n",
        "        self.concat_weight = nn.Linear((args.conv_depth + 1) * args.dim_embs, args.dim_embs, bias=False)\n",
        "\n",
        "        if self.args.conv_depth == 1:\n",
        "            self.sampler_nodes = [5]\n",
        "            self.sampler_inference = [10]\n",
        "        elif self.args.conv_depth == 2:\n",
        "            self.sampler_nodes = [10, 20]\n",
        "            self.sampler_inference = [10 , 20]\n",
        "        else:\n",
        "            raise\n",
        "#         if self.args.conv_depth == 3:\n",
        "#             self.sampler_nodes = [5, 10, 10]\n",
        "#             self.sampler_inference = [10, 10, 10]\n",
        "            \n",
        "        nn.init.xavier_uniform_(self.feature_trans.weight)\n",
        "        \n",
        "        \n",
        "    def calc_from_loader(self, loader, x, device):\n",
        "        y = torch.zeros(len(x), self.args.dim_embs)\n",
        "        attn_res = torch.zeros(len(x), len(self.etypes))\n",
        "        \n",
        "        def calc_from_blocks(blocks, conv_idx, x, device):\n",
        "            input_nodes, output_nodes = blocks[0].srcdata[dgl.NID], blocks[0].dstdata[dgl.NID]\n",
        "            h = x[input_nodes].to(device)\n",
        "            h = torch.tanh(self.feature_trans(h))\n",
        "            for b, idx in zip(blocks, conv_idx):\n",
        "                b = b.to(device)\n",
        "                h, attn = self.convs[idx](b, h)\n",
        "            return h, attn\n",
        "        \n",
        "        for input_nodes, output_nodes, blocks in loader:\n",
        "            \n",
        "            h0 = x[output_nodes].to(device)\n",
        "            h0 = torch.tanh(self.feature_trans(h0))\n",
        "            emb_ulti = [h0]\n",
        "            if self.args.conv_depth == 1:\n",
        "                h1, attn = calc_from_blocks(blocks, [0], x, device)\n",
        "                emb_ulti.append(h1)\n",
        "            if self.args.conv_depth ==2:\n",
        "                h1, _ = calc_from_blocks(blocks[1:], [1], x, device)\n",
        "                h2, attn = calc_from_blocks(blocks, [0, 1], x, device)\n",
        "                emb_ulti.extend([h1, h2])\n",
        "            \n",
        "            y[output_nodes] = self.concat_weight(torch.cat(emb_ulti, dim=-1)).cpu()\n",
        "            attn_res[output_nodes] = attn.squeeze(dim=-1).transpose(0, 1).cpu()\n",
        "        return y, attn_res\n",
        "\n",
        "    def forward(self, g, x, nids, device):\n",
        "        dataloader = dgl.dataloading.DataLoader(g, nids,\n",
        "                                                    dgl.dataloading.MultiLayerNeighborSampler(self.sampler_nodes),\n",
        "                                                    batch_size=self.args.sampling_batch_size,\n",
        "                                                    num_workers=self.args.num_workers,\n",
        "                                                    shuffle=True,\n",
        "                                                    drop_last=False)\n",
        "        y, attn_res = self.calc_from_loader(dataloader, x, device)\n",
        "        return y, attn_res\n",
        "    \n",
        "    \n",
        "    def inference(self, g, x, nids, device):\n",
        "#         dataloader = dgl.dataloading.NodeDataLoader(g, nids,\n",
        "#                                                     dgl.dataloading.MultiLayerFullNeighborSampler(len(self.sampler_nodes)),\n",
        "#                                                     batch_size=self.args.inference_batch_size,\n",
        "#                                                     num_workers=self.args.num_workers,\n",
        "#                                                     shuffle=True,\n",
        "#                                                     drop_last=False)\n",
        "        dataloader = dgl.dataloading.DataLoader(g, nids,\n",
        "                                                    dgl.dataloading.MultiLayerNeighborSampler(self.sampler_inference),\n",
        "                                                    batch_size=self.args.sampling_batch_size,\n",
        "                                                    num_workers=self.args.num_workers,\n",
        "                                                    shuffle=True,\n",
        "                                                    drop_last=False)\n",
        "        y, attn_res = self.calc_from_loader(dataloader, x, device)\n",
        "        return y, attn_res\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZQcYZ-V_OSeR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define Whole Model\n",
        "'''\n",
        "'''\n",
        "This is a PyTorch module definition with a class name Model. \n",
        "The module represents a neural network architecture. \n",
        "The class has four methods: __init__(), forward(), inference(), and compute_contrastive_loss().\n",
        "\n",
        "__init__() method initializes the class by setting up its parameters and neural network architecture.\n",
        "\n",
        "forward() method implements the forward pass of the neural network. \n",
        "It takes four input parameters: g_attr, g_stru, nids, and device. g_attr and g_stru are graph structures with node and edge features. nids is a tensor containing node ids. device is the device on which the computations are performed. The method computes the node embeddings for both graphs and returns them.\n",
        "\n",
        "inference() method is used to generate the embeddings of the nodes using the trained model. \n",
        "It is similar to the forward() method, but it returns the attention scores in addition to the node embeddings.\n",
        "\n",
        "compute_contrastive_loss() method is used to compute the contrastive loss between the embeddings of the positive and negative samples. \n",
        "The method takes the device and node embeddings as inputs and returns the contrastive loss.\n",
        "\n",
        "The model has a ScorePredictor class which predicts the similarity score between two given nodes based on their node embeddings. \n",
        "The HetAttn class computes the node embeddings using heterogeneous attention mechanism. The embeddings of nodes in g_attr and g_stru are computed separately using the pos_emb_model and neg_emb_model if args.sign_conv equals 'sign'. Otherwise, only one emb_model is used. \n",
        "The transform and attention linear layers are used to combine the embeddings of nodes.\n",
        "\n",
        "The model also has some other parameters like dim_embs, combine_type, and sign_aggre, which are used to define the neural network architecture.\n",
        "'''\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        if args.sign_conv == 'sign':\n",
        "            self.pos_emb_model = HetAttn(args, args.pos_edge_type)\n",
        "            self.neg_emb_model = HetAttn(args, args.neg_edge_type)\n",
        "        elif args.sign_conv == 'common':\n",
        "            self.emb_model = HetAttn(args, args.edge_type)\n",
        "        self.args = args\n",
        "        self.link_predictor = ScorePredictor(args, dim_embs = args.dim_embs)\n",
        "        \n",
        "        self.combine_type = args.combine_type\n",
        "        \n",
        "        if self.args.sign_aggre!='both':\n",
        "            transform_type = 2\n",
        "        elif self.args.sign_aggre == 'both' or self.args.sign_conv == 'common':\n",
        "            transform_type = 4\n",
        "        \n",
        "        if self.combine_type == 'concat':\n",
        "            self.transform = nn.Sequential(nn.Linear(transform_type*args.dim_embs, args.dim_embs))   #transform\n",
        "            self.link_predictor = ScorePredictor(args, dim_embs = args.dim_embs)\n",
        "        elif self.combine_type == 'attn':\n",
        "            self.attention = nn.Sequential(nn.Linear(args.dim_embs, args.dim_query), nn.Tanh(), nn.Linear(args.dim_query, 1, bias=False))\n",
        "            self.link_predictor = ScorePredictor(args, dim_embs=args.dim_embs)\n",
        "        \n",
        "    def forward(self, g_attr, g_stru, nids, device):\n",
        "#         nids = torch.unique(torch.cat((uids, vids), dim=-1))\n",
        "#         embs_pos, embs_neg = self.emb_model(g, x, nids, device)\n",
        "#         score = self.predict_combine((embs_pos, embs_neg), uids, vids, device)\n",
        "        if self.args.sign_conv == 'common':\n",
        "            embs_attr_pos, _ = self.emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, _ = self.emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, _ = self.emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, _ = self.emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            return embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg\n",
        "        elif self.args.sign_conv == 'sign':\n",
        "            embs_attr_pos, _ = self.pos_emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, _ = self.neg_emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, _ = self.pos_emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, _ = self.neg_emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            return embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg\n",
        "    \n",
        "    def inference(self, g_attr, g_stru, nids, device):\n",
        "        if self.args.sign_conv == 'common':\n",
        "            embs_attr_pos, attn_attr_pos = self.emb_model(g_attr, g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, attn_attr_neg = self.emb_model(g_attr, g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, attn_stru_pos = self.emb_model(g_stru, g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, attn_stru_neg = self.emb_model(g_stru, g_stru.ndata['feature'], nids, device)\n",
        "            return (embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg), (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg)\n",
        "        elif self.args.sign_conv == 'sign':\n",
        "            embs_attr_pos, attn_attr_pos = self.pos_emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, attn_attr_neg = self.neg_emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, attn_stru_pos = self.pos_emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, attn_stru_neg = self.neg_emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            print()\n",
        "            return (embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg), (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg)\n",
        "\n",
        "        \n",
        "    def compute_contrastive_loss(self, device, embs_attr_pos, embs_stru_pos, embs_attr_neg=None, embs_stru_neg=None):\n",
        "        nodes_num = embs_attr_pos.shape[0]\n",
        "        feature_size = embs_attr_pos.shape[1]\n",
        "        \n",
        "        embs_attr_pos = embs_attr_pos.to(device)\n",
        "        embs_stru_pos = embs_stru_pos.to(device)\n",
        "        normalized_embs_attr_pos = F.normalize(embs_attr_pos, p=2, dim=1)\n",
        "        normalized_embs_stru_pos = F.normalize(embs_stru_pos, p=2, dim=1)\n",
        "        if embs_attr_neg!=None and embs_stru_neg!=None:\n",
        "            embs_attr_neg = embs_attr_neg.to(device)\n",
        "            embs_stru_neg = embs_stru_neg.to(device)\n",
        "            normalized_embs_attr_neg = F.normalize(embs_attr_neg, p=2, dim=1)\n",
        "            normalized_embs_stru_neg = F.normalize(embs_stru_neg, p=2, dim=1)\n",
        "        \n",
        "        \n",
        "        def inter_contrastive(embs_attr, embs_stru):\n",
        "            pos = torch.exp(torch.div(torch.bmm(embs_attr.view(nodes_num, 1, feature_size), embs_stru.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            \n",
        "            def generate_neg_score(embs_1, embs_2):\n",
        "                neg_similarity = torch.mm(embs_1.view(nodes_num, feature_size), embs_2.transpose(0,1))\n",
        "                neg_similarity[np.arange(nodes_num),np.arange(nodes_num)] = 0\n",
        "                return torch.sum(torch.exp(torch.div( neg_similarity  , self.args.tao)) , dim=1)\n",
        "            \n",
        "            neg = generate_neg_score(embs_attr, embs_stru)\n",
        "\n",
        "            return torch.mean(- (torch.log(torch.div(pos, neg))))\n",
        "        \n",
        "        def intra_contrastive(self_embs, embs_attr_pos, embs_attr_neg, embs_stru_pos, embs_stru_neg):\n",
        "            pos_score_1 = torch.exp(torch.div(torch.bmm(self_embs.view(nodes_num, 1, feature_size), embs_attr_pos.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            pos_score_2 = torch.exp(torch.div(torch.bmm(self_embs.view(nodes_num, 1, feature_size), embs_stru_pos.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            pos = pos_score_1 + pos_score_2\n",
        "            def generate_neg_score(pos_embs, neg_embs_1, neg_embs_2):\n",
        "                neg_score_1 = torch.bmm(pos_embs.view(nodes_num, 1, feature_size), neg_embs_1.view(nodes_num, feature_size, 1))\n",
        "                neg_score_2 = torch.bmm(pos_embs.view(nodes_num, 1, feature_size), neg_embs_2.view(nodes_num, feature_size, 1))\n",
        "                return torch.exp(torch.div(neg_score_1, self.args.tao)) + torch.exp(torch.div(neg_score_2, self.args.tao))\n",
        "            neg = generate_neg_score(self_embs, embs_attr_neg, embs_stru_neg)\n",
        "            return torch.mean(- torch.log(torch.div(pos, neg)) )\n",
        "            \n",
        "\n",
        "        inter_pos = inter_contrastive(normalized_embs_attr_pos, normalized_embs_stru_pos)\n",
        "        inter_neg = inter_contrastive(normalized_embs_attr_neg, normalized_embs_stru_neg)\n",
        "        \n",
        "        embs = torch.cat((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), dim=-1)\n",
        "        # print(\"#####################################\")\n",
        "        # print(\"embs_attr_pos\")\n",
        "        # print(embs_attr_pos.size())\n",
        "        # print(\"embs_stru_pos\")\n",
        "        # print(embs_stru_pos.size())\n",
        "        # print(\"embs_attr_neg\")\n",
        "        # print(embs_attr_neg.size())\n",
        "        # print(\"embs_stru_neg\")\n",
        "        # print(embs_stru_neg.size())\n",
        "   \n",
        "        # print(\"concatenated emb of ultimate representation\")\n",
        "        # print(embs.size())\n",
        "        self_embs = self.transform(embs)\n",
        "        # print(\"transformed emb of ultimate representation \")\n",
        "        # print(self_embs.size())\n",
        "        normalized_self_embs = F.normalize(self_embs, p=2, dim=1)\n",
        "        # print(\"normalized emb of ultimate representation \")\n",
        "        # print(normalized_self_embs.size())\n",
        "\n",
        "        \n",
        "        intra = intra_contrastive(normalized_self_embs, normalized_embs_attr_pos, normalized_embs_attr_neg, normalized_embs_stru_pos, normalized_embs_stru_neg)\n",
        "        # print(f'inter_pos:{inter_pos}  inter_neg:{inter_neg}  intra:{intra}')\n",
        "        if self.args.contrast_type == 'pos':\n",
        "            return inter_pos\n",
        "        elif self.args.contrast_type == 'neg':\n",
        "            return inter_neg\n",
        "        elif self.args.contrast_type == 'intra':\n",
        "            return intra\n",
        "        elif self.args.contrast_type == 'inter':\n",
        "            return inter_pos + inter_neg\n",
        "        elif self.args.contrast_type == 'all':\n",
        "            return (1-self.args.beta) * (inter_pos + inter_neg) + self.args.beta * intra\n",
        "            \n",
        "\n",
        "        \n",
        "    \n",
        "    def compute_label_loss(self, score, y_label, pos_weight, device):\n",
        "        pos_weight = torch.tensor([(y_label==0).sum().item()/(y_label==1).sum().item()]*y_label.shape[0]).to(device)\n",
        "        return F.binary_cross_entropy_with_logits(score, y_label, pos_weight=pos_weight)\n",
        "    \n",
        "        \n",
        "        \n",
        "    def predict_combine(self, embs, uids, vids, device):\n",
        "        u_embs = self.combine(embs, uids, device)\n",
        "        v_embs = self.combine(embs, vids, device)\n",
        "        score = self.link_predictor(u_embs, v_embs)\n",
        "        return score\n",
        "    \n",
        "    def compute_attention(self, embs):\n",
        "        attn = self.attention(embs).softmax(dim=0)\n",
        "        return attn\n",
        "    \n",
        "    def combine(self, embs, nids, device):\n",
        "        if self.args.sign_conv == 'sign':\n",
        "            if self.args.sign_aggre == 'pos':\n",
        "                embs = (embs[0],embs[1])\n",
        "            elif self.args.sign_aggre == 'neg':\n",
        "                embs = (embs[2],embs[3])\n",
        "            \n",
        "        if self.combine_type == 'concat':\n",
        "            embs = torch.cat(embs, dim=-1)\n",
        "            sub_embs = embs[nids].to(device)\n",
        "            out_embs = self.transform(sub_embs)\n",
        "            return out_embs                          #output embs\n",
        "        elif self.combine_type == 'attn':\n",
        "            embs = torch.stack(embs, dim=0)\n",
        "            sub_embs = embs[:,nids].to(device)\n",
        "            attn = self.compute_attention(sub_embs)\n",
        "            # attn: (2,n,1)   sub_embs: (2,n,feature)\n",
        "            out_embs = (attn*sub_embs).sum(dim=0)\n",
        "            return out_embs\n",
        "        elif self.combine_type == 'mean':\n",
        "            embs = torch.stack(embs, dim=0).mean(dim=0)\n",
        "            sub_embs = embs[nids].to(device)\n",
        "            return sub_embs\n",
        "        elif self.combine_type == 'pos':\n",
        "            sub_embs = embs[0][nids].to(device)\n",
        "            return sub_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OPyI8dHHzICO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scpN3BUzxsd"
      },
      "source": [
        "# Graph Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "sEjGcQ2SOzjM"
      },
      "outputs": [],
      "source": [
        "############################################### Graph Augmentation ############################################################\n",
        "\"\"\"\n",
        "This code contains a series of functions that augment a graph by adding noise. \n",
        "The code is written in Python and uses the DGL (Deep Graph Library) library for manipulating the graph.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "This generate_mask function generates a mask for the graph. \n",
        "The mask_ratio parameter determines the proportion of the mask that is filled with 0s (elements to drop) or 1s (elements to leave). \n",
        "The row and column parameters specify the dimensions of the mask. \n",
        "The function generates a random array of floats between 0 and 1 of the specified dimensions, masks the values below the mask_ratio parameter with 0s, and the rest with 1s.\n",
        "\n",
        "\"\"\"\n",
        "def generate_mask(mask_ratio, row, column):\n",
        "    # 1 -- leave   0 -- drop\n",
        "    arr_mask_ratio = np.random.uniform(0,1,size=(row, column))\n",
        "    arr_mask = np.ma.masked_array(arr_mask_ratio, mask=(arr_mask_ratio<mask_ratio)).filled(0)\n",
        "    arr_mask = np.ma.masked_array(arr_mask, mask=(arr_mask>=mask_ratio)).filled(1)\n",
        "    return arr_mask\n",
        "\n",
        "\"\"\"\n",
        "generate_attr_graph function generates noise for the graph features (node attributes) by adding random noise and dropping some elements using the mask generated by generate_mask(). \n",
        "The g parameter is the input graph, and args is a collection of arguments for the augmentation. \n",
        "The function generates random noise by sampling from a normal distribution with a mean of 0 and a standard deviation of 0.1. \n",
        "Then it applies the mask to the feature matrix, element-wise, and adds the noise to the remaining elements. \n",
        "The function returns a new graph with the noisy features.\n",
        "\"\"\"    \n",
        "\n",
        "def generate_attr_graph(g, args):\n",
        "    # generate noise g_attr\n",
        "    feature = g.ndata['feature']\n",
        "    attr_noise = np.random.normal(loc=0, scale=0.1, size=(feature.shape[0], feature.shape[1]))\n",
        "    attr_mask = generate_mask(args.mask_ratio, row=feature.shape[0], column=feature.shape[1])\n",
        "    noise_feature = feature*attr_mask + (1-attr_mask) * attr_noise\n",
        "    \n",
        "    g_attr = g\n",
        "    g_attr.ndata['feature'] = noise_feature.float()\n",
        "    return g_attr\n",
        "\n",
        "\"\"\"\n",
        "function generate_stru_graph takes a graph g and some arguments args. \n",
        "It creates a copy of the graph g_stru and deletes a certain percentage of edges of specific types specified in args. It then adds an equal number of randomly sampled edges back into the graph, ensuring that they don't already exist in the original graph.\n",
        "Finally, it casts the node features to float and returns the augmented graph.\n",
        "\"\"\"\n",
        "def generate_stru_graph(g, args):\n",
        "    # generate noise g_stru by deleting links\n",
        "    g_stru = g\n",
        "\n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "        \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        # shape: (e, 2)\n",
        "        df = np.array([etype_edges[0].numpy(), etype_edges[1].numpy()]).transpose()\n",
        "        \n",
        "        # delete edges\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "\n",
        "        # add an equal number of edges\n",
        "        add_row = []\n",
        "        add_column = []\n",
        "        index = 0\n",
        "        while index < len(drop_eids):\n",
        "            row_sample = np.random.randint(g.num_nodes())\n",
        "            column_sample = np.random.randint(g.num_nodes())\n",
        "            if (df==[row_sample, column_sample]).all(1).any() == False:\n",
        "                index += 1\n",
        "                add_row.append(row_sample)\n",
        "                add_column.append(column_sample)\n",
        "        g_stru = dgl.add_edges(g_stru, add_row, add_column, etype=etype)\n",
        "\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\n",
        "\"\"\"\n",
        " function generate_stru_sign_graph is similar to the first, but instead of adding new edges, it exchanges some positive or negative edges with randomly selected edges of the opposite sign.\n",
        "\"\"\"\n",
        "def generate_stru_sign_graph(g, args):\n",
        "    # generate noise g_stru by exchanging some pos/neg links\n",
        "    g_stru = g\n",
        "    \n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "    \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        \n",
        "        # delete edges\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "        \n",
        "        # add_edges\n",
        "        if etype in args.pos_edge_type:\n",
        "            g_stru = dgl.add_edges(g_stru, etype_edges[0][drop_eids], etype_edges[1][drop_eids] , etype=random.choice(args.neg_edge_type))\n",
        "        elif etype in args.neg_edge_type:\n",
        "            g_stru = dgl.add_edges(g_stru, etype_edges[0][drop_eids], etype_edges[1][drop_eids] , etype=random.choice(args.pos_edge_type))\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\"\"\"\n",
        " function generate_stru_status_graph deletes edges of specific types as before, but instead of adding new edges, it adds reverse edges in their place.\n",
        "\"\"\"\n",
        "def generate_stru_status_graph(g, args):\n",
        "    g_stru = g\n",
        "    \n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "    \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        \n",
        "        # delete edges\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "        \n",
        "        # add reverse_edges\n",
        "        g_stru = dgl.add_edges(g_stru, etype_edges[1][drop_eids], etype_edges[0][drop_eids], etype=etype)\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\"\"\"\n",
        "function GraphAug takes a graph g and some arguments args. \n",
        "It calls one of the above functions based on the specified args.augment parameter and returns two augmented graphs, one with attribute perturbations g_attr and one with structural perturbations g_stru.\n",
        "\n",
        "If args.augment is 'delete', it calls generate_stru_graph twice. \n",
        "If args.augment is 'change', it calls generate_stru_sign_graph twice. \n",
        "If args.augment is 'reverse', it calls generate_stru_status_graph twice. \n",
        "If args.augment is 'composite', it calls generate_stru_sign_graph once and generate_stru_graph once, returning one graph with sign perturbations and one with attribute perturbations.\n",
        "\"\"\"\n",
        "def GraphAug(g, args):\n",
        "    if args.augment == 'delete':     #for connectivity perturbation\n",
        "        g_attr = generate_stru_graph(g, args)\n",
        "        g_stru = generate_stru_graph(g, args)\n",
        "    elif args.augment == 'change':          #for sign perturbation\n",
        "        g_attr = generate_stru_sign_graph(g, args)\n",
        "        g_stru = generate_stru_sign_graph(g, args)\n",
        "    elif args.augment == 'reverse':\n",
        "        g_attr = generate_stru_status_graph(g, args)\n",
        "        g_stru = generate_stru_status_graph(g, args)\n",
        "    elif args.augment == 'composite':\n",
        "        g_attr = generate_stru_sign_graph(g, args)\n",
        "        g_stru = generate_stru_graph(g, args)\n",
        "    return g_attr, g_stru\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVXqMuazI8e"
      },
      "source": [
        "# Define Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TSSanEGOOnI3"
      },
      "outputs": [],
      "source": [
        "############################################### Define Predictor ############################################################\n",
        "\"\"\"\n",
        "This code defines a Python class named ScorePredictor, which is a subclass of the PyTorch nn.Module class. \n",
        "The class is used for predicting a score given two embeddings u_e and u_v. \n",
        "The class constructor takes two parameters - args, which is an instance of the DotMap class, and params, which is a dictionary containing additional parameters. \n",
        "The constructor initializes the instance variables and sets them to the DotMap object created from args.toDict().\n",
        "\n",
        "The constructor also contains a conditional block that sets the predictor instance variable depending on the value of args.predictor. \n",
        "If args.predictor is 'dot', then predictor is set to None. \n",
        "Otherwise, predictor is set to a PyTorch sequential neural network consisting of one or more linear layers with leaky ReLU activation. \n",
        "The number of linear layers and the number of hidden units in each linear layer are determined by args.predictor.\n",
        "\n",
        "The class contains a method named reset_parameters, which does nothing. \n",
        "The forward method takes u_e and u_v embeddings as input and returns the predicted score based on the predictor instance variable.\n",
        "\n",
        "The next method defined is eval_model. \n",
        "This method takes four parameters - embs, which are the embeddings, model, which is an instance of the ScorePredictor class, df, which is a Pandas DataFrame, and batched, which is a boolean value indicating whether to use batching or not. \n",
        "The method creates a PyTorch DataLoader object to iterate over the input data, and returns the predicted and true labels.\n",
        "\n",
        "The last method defined is eval_metric. \n",
        "This method takes six parameters - embs, model, df, args, device, and an optional threshold value. \n",
        "The method calls the eval_model method to get predicted and true labels, and then computes various evaluation metrics such as AUC, precision, recall, and F1 scores using the sklearn.metrics library. \n",
        "The method returns these evaluation metrics. The threshold value is used to convert the predicted scores into binary labels, and the specific value of the threshold may depend on the particular dataset being used.\n",
        "\"\"\"\n",
        "\n",
        "class ScorePredictor(nn.Module):\n",
        "    def __init__(self, args, **params):\n",
        "        super().__init__()\n",
        "        self.args = DotMap(args.toDict())\n",
        "        for k,v in params.items():\n",
        "            self.args[k] = v\n",
        "        \n",
        "        if self.args.predictor == 'dot':\n",
        "            pass\n",
        "        elif self.args.predictor == '1-linear':\n",
        "            self.predictor = nn.Linear(self.args.dim_embs*2, 1)\n",
        "        elif self.args.predictor == '2-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1))\n",
        "        elif self.args.predictor == '3-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1)\n",
        "                                         )\n",
        "        elif self.args.predictor == '4-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1)\n",
        "                                         )\n",
        "        self.reset_parameters()\n",
        "            \n",
        "    def reset_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, u_e, u_v):\n",
        "        if self.args.predictor == 'dot':\n",
        "            score = u_e.mul(u_v).sum(dim=-1)\n",
        "        else:\n",
        "            x = torch.cat([u_e, u_v], dim=-1)\n",
        "            score = self.predictor(x).flatten()\n",
        "        return score\n",
        "\n",
        "def eval_model(embs, model, df, batched, args, device):\n",
        "    if batched:\n",
        "        dataset = LabelPairs(df)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.loss_batch_size, num_workers=args.num_workers, shuffle=True)\n",
        "        y_pre_list = []\n",
        "        y_true_list = []\n",
        "        for pair, y in dataloader:\n",
        "            uids, vids = pair.T\n",
        "            score  = model.predict_combine(embs, uids, vids, device)\n",
        "            y_pre_list.append(torch.sigmoid(score))\n",
        "            y_true_list.append(y)\n",
        "        y_pre = torch.cat(y_pre_list, dim=-1).cpu().numpy()\n",
        "        y_true = torch.cat(y_true_list, dim=-1).cpu().numpy()\n",
        "    else:\n",
        "        uids = torch.from_numpy(df.src.values).long()\n",
        "        vids = torch.from_numpy(df.dst.values).long()\n",
        "        score  = model.predict_combine(embs, uids, vids, device)\n",
        "        y_pre = torch.sigmoid(score).cpu().numpy()\n",
        "        y_true = df['label'].values\n",
        "    return y_true, y_pre\n",
        "    \n",
        "def eval_metric(embs, model, df, args, device, threshold=0.05):\n",
        "\t# change threshold according to different datasets\n",
        "\t# 0.05 for Alpha, 0.1 for OTC\n",
        "    y_true, y_pre = eval_model(embs, model, df, args.eval_batched, args, device)\n",
        "    y = (y_pre > threshold)\n",
        "    auc = metrics.roc_auc_score(y_true, y_pre)\n",
        "    prec = metrics.precision_score(y_true, y)\n",
        "    recl = metrics.recall_score(y_true, y)\n",
        "    binary_f1 = metrics.f1_score(y_true, y, average='binary')\n",
        "    micro_f1 = metrics.f1_score(y_true, y, average='micro')\n",
        "    macro_f1 = metrics.f1_score(y_true, y, average='macro')\n",
        "    \n",
        "    \n",
        "    return auc, prec, recl, micro_f1, binary_f1, macro_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "LCt-jdwfzMK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ygezuIzYHu"
      },
      "source": [
        "# Training Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "uXzmCBq4OsHG"
      },
      "outputs": [],
      "source": [
        "############################################### Training Parameter Setting ############################################################\n",
        "\n",
        "args = DotMap()\n",
        "args.num_nodes = graph_user.num_nodes()\n",
        "\n",
        "args.pos_edge_type = ['positive']\n",
        "args.neg_edge_type = ['negative']\n",
        "args.edge_type = args.pos_edge_type+args.neg_edge_type\n",
        "args.num_edge_types = len(args.edge_type)\n",
        "args.dim_features = graph_user.nodes['user'].data['feature'].shape[1]\n",
        "args.dim_hiddens = 128\n",
        "args.dim_embs = 128\n",
        "\n",
        "args.learning_rate = 0.01\n",
        "\n",
        "args.conv_depth = 2\n",
        "args.loss_batch_size = 102400             # to calculated loss\n",
        "\n",
        "#args.inference_batch_size = 128       # the batch size for inferencing all/batched nodes embeddings\n",
        "args.sampling_batch_size = 128\n",
        "args.residual = False\n",
        "args.num_heads = 8\n",
        "args.dropout = 0\n",
        "\n",
        "# active_tag walktogether_tag  friend_tag  playagain_tag \n",
        "# label\n",
        "args.label = 'label'  \n",
        "args.conv_type = 'gat'\n",
        "args.het_agg_type = 'attn' # multiplex aggregation\n",
        "args.dim_query = 128\n",
        "args.predictor = '2-linear'\n",
        "# concat / mean / attn / pos\n",
        "args.combine_type = 'concat'\n",
        "\n",
        "# sign / common\n",
        "args.sign_conv = 'sign'\n",
        "# pos / neg / both\n",
        "args.sign_aggre = 'both'\n",
        "# pos / neg / intra / inter / all\n",
        "args.contrast_type = 'all'\n",
        "# delete / change / reverse / composite\n",
        "args.augment = 'change'\n",
        "\n",
        "#args.contrastive = True\n",
        "args.mask_ratio = 0.1\n",
        "args.tao = 0.05\n",
        "args.alpha = 1e-4\n",
        "args.beta = 0.8\n",
        "args.pos_gamma = 1\n",
        "args.neg_gamma = 1\n",
        "\n",
        "args.gpu = 0\n",
        "args.num_workers = 0\n",
        "args.verbose = 1\n",
        "args.pretrain_epochs = 101\n",
        "args.finetune_epochs = 0\n",
        "# both / pos / neg\n",
        "args.drop_type = 'both'\n",
        "\n",
        "# 2-layer 20\n",
        "\n",
        "device = torch.device(f'cuda:{args.gpu}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "0FesTBfSnpsR"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "mWnquZhYnv8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "jokLF32knQzh"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/bitcoin_alpha/label_train.pkl\", \"rb\") as f:\n",
        "    label_train = pickle.load(f)\n",
        "    label_test =  label_train\n",
        "    \n",
        "label_ids = np.unique(np.concatenate((label_train.src, label_train.dst, label_test.src, label_test.dst)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "owJm5UUv0V6Z"
      },
      "outputs": [],
      "source": [
        "# label_train.to_csv(\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/bitcoin_alpha/bitcoin_alpha_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Gt-0BeCLIeyr",
        "outputId": "349c0583-0f18-447d-b545-31cefb7dda6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-64c329b3-9072-4939-bac8-d1f78b88f00b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>dst</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>398</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>398</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>398</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>398</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>398</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24181</th>\n",
              "      <td>1064</td>\n",
              "      <td>1061</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24182</th>\n",
              "      <td>1061</td>\n",
              "      <td>1064</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24183</th>\n",
              "      <td>1064</td>\n",
              "      <td>1060</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24184</th>\n",
              "      <td>1060</td>\n",
              "      <td>1064</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24185</th>\n",
              "      <td>1064</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24186 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64c329b3-9072-4939-bac8-d1f78b88f00b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64c329b3-9072-4939-bac8-d1f78b88f00b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64c329b3-9072-4939-bac8-d1f78b88f00b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        src   dst  label\n",
              "0         0   398      1\n",
              "1         1   398      1\n",
              "2         2   398      1\n",
              "3         3   398      1\n",
              "4         4   398      1\n",
              "...     ...   ...    ...\n",
              "24181  1064  1061      1\n",
              "24182  1061  1064      1\n",
              "24183  1064  1060      1\n",
              "24184  1060  1064      1\n",
              "24185  1064   132      0\n",
              "\n",
              "[24186 rows x 3 columns]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2=label_train\n",
        "type(label_train)\n",
        "label_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZEW9Ykmz3gx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zMdZkdWg6Ns",
        "outputId": "21095519-e9e3-4d80-f89c-3cfa545a459d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:0  0/1  loss_contrastive:2.663815975189209  loss_label:0.08836776597027415.\n",
            "epoch:1  0/1  loss_contrastive:-5.190564155578613  loss_label:0.08624649072788167.\n",
            "epoch:2  0/1  loss_contrastive:-9.614057540893555  loss_label:0.08381217624767313.\n",
            "epoch:3  0/1  loss_contrastive:-11.335478782653809  loss_label:0.07953752925372724.\n",
            "epoch:4  0/1  loss_contrastive:-12.382535934448242  loss_label:0.07379129285048108.\n",
            "epoch:5  0/1  loss_contrastive:-12.626639366149902  loss_label:0.06935217668532494.\n",
            "epoch:6  0/1  loss_contrastive:-12.768013000488281  loss_label:0.07796604453742408.\n",
            "epoch:7  0/1  loss_contrastive:-12.80050277709961  loss_label:0.11350380217285538.\n",
            "epoch:8  0/1  loss_contrastive:-12.726081848144531  loss_label:0.08481967811176208.\n",
            "epoch:9  0/1  loss_contrastive:-12.607086181640625  loss_label:0.07202102807405245.\n",
            "epoch:10  0/1  loss_contrastive:-12.313714027404785  loss_label:0.09122072795581797.\n",
            "epoch:11  0/1  loss_contrastive:-12.031588554382324  loss_label:0.0717131821365382.\n",
            "epoch:12  0/1  loss_contrastive:-11.570172309875488  loss_label:0.06948106708345314.\n",
            "epoch:13  0/1  loss_contrastive:-10.979093551635742  loss_label:0.07774010658381184.\n",
            "epoch:14  0/1  loss_contrastive:-10.316191673278809  loss_label:0.07792871199858405.\n",
            "epoch:15  0/1  loss_contrastive:-9.725613594055176  loss_label:0.07350746543677258.\n",
            "epoch:16  0/1  loss_contrastive:-9.036460876464844  loss_label:0.07089757630308409.\n",
            "epoch:17  0/1  loss_contrastive:-8.424824714660645  loss_label:0.07202952461994018.\n",
            "epoch:18  0/1  loss_contrastive:-8.129526138305664  loss_label:0.07447617072570846.\n",
            "epoch:19  0/1  loss_contrastive:-8.126157760620117  loss_label:0.07487693065291615.\n",
            "epoch:20  0/1  loss_contrastive:-8.176143646240234  loss_label:0.07244573693890251.\n",
            "epoch:21  0/1  loss_contrastive:-8.360318183898926  loss_label:0.07081513800187249.\n",
            "epoch:22  0/1  loss_contrastive:-8.748477935791016  loss_label:0.06865517673315263.\n",
            "epoch:23  0/1  loss_contrastive:-9.023233413696289  loss_label:0.06975632726365953.\n",
            "epoch:24  0/1  loss_contrastive:-9.309907913208008  loss_label:0.06860629496548831.\n",
            "epoch:25  0/1  loss_contrastive:-9.586021423339844  loss_label:0.06982209251639294.\n",
            "epoch:26  0/1  loss_contrastive:-9.85440731048584  loss_label:0.06638495340726329.\n",
            "epoch:27  0/1  loss_contrastive:-9.911674499511719  loss_label:0.06610930367962002.\n",
            "epoch:28  0/1  loss_contrastive:-10.37690544128418  loss_label:0.06837040610005839.\n",
            "epoch:29  0/1  loss_contrastive:-10.665189743041992  loss_label:0.06556966703163286.\n",
            "epoch:30  0/1  loss_contrastive:-10.829776763916016  loss_label:0.06582126821810974.\n",
            "epoch:31  0/1  loss_contrastive:-11.005905151367188  loss_label:0.06358969034168929.\n",
            "epoch:32  0/1  loss_contrastive:-11.218515396118164  loss_label:0.06496256168063631.\n",
            "epoch:33  0/1  loss_contrastive:-11.570853233337402  loss_label:0.0646898056613473.\n",
            "epoch:34  0/1  loss_contrastive:-11.877106666564941  loss_label:0.06441713104454047.\n",
            "epoch:35  0/1  loss_contrastive:-12.06753158569336  loss_label:0.06620898383677709.\n",
            "epoch:36  0/1  loss_contrastive:-12.30427360534668  loss_label:0.06520832133663249.\n",
            "epoch:37  0/1  loss_contrastive:-12.608530044555664  loss_label:0.06300142725425319.\n",
            "epoch:38  0/1  loss_contrastive:-12.715835571289062  loss_label:0.06203759933151004.\n",
            "epoch:39  0/1  loss_contrastive:-13.055036544799805  loss_label:0.06379933003692657.\n",
            "epoch:40  0/1  loss_contrastive:-13.306985855102539  loss_label:0.062025333193261215.\n",
            "epoch:41  0/1  loss_contrastive:-13.555715560913086  loss_label:0.06158086004245531.\n",
            "epoch:42  0/1  loss_contrastive:-13.646955490112305  loss_label:0.06179249872805809.\n",
            "epoch:43  0/1  loss_contrastive:-14.021902084350586  loss_label:0.06323372052336358.\n",
            "epoch:44  0/1  loss_contrastive:-14.27784538269043  loss_label:0.061153716930488.\n",
            "epoch:45  0/1  loss_contrastive:-14.540724754333496  loss_label:0.06187891815747584.\n",
            "epoch:46  0/1  loss_contrastive:-14.787692070007324  loss_label:0.05877162138578085.\n",
            "epoch:47  0/1  loss_contrastive:-14.99010181427002  loss_label:0.061813841640437815.\n",
            "epoch:48  0/1  loss_contrastive:-15.12466049194336  loss_label:0.060797722678863964.\n",
            "epoch:49  0/1  loss_contrastive:-15.507733345031738  loss_label:0.061290582064337774.\n",
            "epoch:49  1/1  loss_contrastive:-15.507733345031738  loss_label:0.061290582064337774.\n",
            "\n",
            "Epoch 49, 1/1.\n",
            "Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.8860, 0.9396, 1.0000, 0.9398, 0.9688, 0.5333)\n",
            "Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.8860, 0.9396, 1.0000, 0.9398, 0.9688, 0.5333)\n",
            "epoch:50  0/1  loss_contrastive:-15.73967456817627  loss_label:0.05958779595037831.\n",
            "epoch:51  0/1  loss_contrastive:-15.912894248962402  loss_label:0.059385146284445516.\n",
            "epoch:52  0/1  loss_contrastive:-16.093008041381836  loss_label:0.05879009239652104.\n",
            "epoch:53  0/1  loss_contrastive:-16.321144104003906  loss_label:0.060730353732840414.\n",
            "epoch:54  0/1  loss_contrastive:-16.5596866607666  loss_label:0.05829490176205975.\n",
            "epoch:55  0/1  loss_contrastive:-16.789487838745117  loss_label:0.060361170698257124.\n",
            "epoch:56  0/1  loss_contrastive:-17.004796981811523  loss_label:0.05701930082773302.\n",
            "epoch:57  0/1  loss_contrastive:-17.071308135986328  loss_label:0.05871214057481082.\n",
            "epoch:58  0/1  loss_contrastive:-17.24395179748535  loss_label:0.057946267712897154.\n",
            "epoch:59  0/1  loss_contrastive:-17.422792434692383  loss_label:0.05823270376918072.\n",
            "epoch:60  0/1  loss_contrastive:-17.701927185058594  loss_label:0.05885747184607865.\n",
            "epoch:61  0/1  loss_contrastive:-17.872791290283203  loss_label:0.05731967474360633.\n",
            "epoch:62  0/1  loss_contrastive:-17.98493766784668  loss_label:0.05707100519898175.\n",
            "epoch:63  0/1  loss_contrastive:-18.252668380737305  loss_label:0.0577980618066348.\n",
            "epoch:64  0/1  loss_contrastive:-18.470937728881836  loss_label:0.057148212885749276.\n",
            "epoch:65  0/1  loss_contrastive:-18.531455993652344  loss_label:0.057068601839584676.\n",
            "epoch:66  0/1  loss_contrastive:-18.781057357788086  loss_label:0.0566845972307232.\n",
            "epoch:67  0/1  loss_contrastive:-18.935083389282227  loss_label:0.0560966654877784.\n",
            "epoch:68  0/1  loss_contrastive:-18.979965209960938  loss_label:0.05604817988675101.\n",
            "epoch:69  0/1  loss_contrastive:-19.36130142211914  loss_label:0.05851373710701231.\n",
            "epoch:70  0/1  loss_contrastive:-19.187637329101562  loss_label:0.057821606844848585.\n",
            "epoch:71  0/1  loss_contrastive:-19.71036720275879  loss_label:0.05629487390426214.\n",
            "epoch:72  0/1  loss_contrastive:-19.703344345092773  loss_label:0.05494454333625114.\n",
            "epoch:73  0/1  loss_contrastive:-19.890888214111328  loss_label:0.054412874863903865.\n",
            "epoch:74  0/1  loss_contrastive:-20.013782501220703  loss_label:0.05523682839313183.\n",
            "epoch:75  0/1  loss_contrastive:-20.074832916259766  loss_label:0.05566641307989373.\n",
            "epoch:76  0/1  loss_contrastive:-20.25421905517578  loss_label:0.054741895157654756.\n",
            "epoch:77  0/1  loss_contrastive:-20.206769943237305  loss_label:0.05595729574062623.\n",
            "epoch:78  0/1  loss_contrastive:-20.554338455200195  loss_label:0.05627684132495408.\n",
            "epoch:79  0/1  loss_contrastive:-20.360713958740234  loss_label:0.054255723510023765.\n",
            "epoch:80  0/1  loss_contrastive:-20.5992374420166  loss_label:0.05376794444568808.\n",
            "epoch:81  0/1  loss_contrastive:-20.64535903930664  loss_label:0.051805941888016786.\n",
            "epoch:82  0/1  loss_contrastive:-20.69272804260254  loss_label:0.053103464860599435.\n",
            "epoch:83  0/1  loss_contrastive:-20.940176010131836  loss_label:0.05430947191131594.\n",
            "epoch:84  0/1  loss_contrastive:-20.77229118347168  loss_label:0.05703678747498168.\n",
            "epoch:85  0/1  loss_contrastive:-21.05575942993164  loss_label:0.05664030354621667.\n",
            "epoch:86  0/1  loss_contrastive:-20.898218154907227  loss_label:0.05320110174592459.\n",
            "epoch:87  0/1  loss_contrastive:-20.983232498168945  loss_label:0.05207821449526031.\n",
            "epoch:88  0/1  loss_contrastive:-21.250951766967773  loss_label:0.05509362385539781.\n",
            "epoch:89  0/1  loss_contrastive:-21.014362335205078  loss_label:0.05557425627834686.\n",
            "epoch:90  0/1  loss_contrastive:-21.215795516967773  loss_label:0.051765093559793626.\n",
            "epoch:91  0/1  loss_contrastive:-21.25520133972168  loss_label:0.053155975887404575.\n",
            "epoch:92  0/1  loss_contrastive:-21.172779083251953  loss_label:0.05578496082947123.\n",
            "epoch:93  0/1  loss_contrastive:-21.388532638549805  loss_label:0.052373023837214656.\n",
            "epoch:94  0/1  loss_contrastive:-21.343585968017578  loss_label:0.05140354009737703.\n",
            "epoch:95  0/1  loss_contrastive:-21.28816795349121  loss_label:0.0514505145812376.\n",
            "epoch:96  0/1  loss_contrastive:-21.439546585083008  loss_label:0.05091684344626404.\n",
            "epoch:97  0/1  loss_contrastive:-21.44843292236328  loss_label:0.05248459568068625.\n",
            "epoch:98  0/1  loss_contrastive:-21.37574577331543  loss_label:0.0528991830528341.\n",
            "epoch:99  0/1  loss_contrastive:-21.579280853271484  loss_label:0.05217844122839169.\n",
            "epoch:99  1/1  loss_contrastive:-21.579280853271484  loss_label:0.05217844122839169.\n",
            "\n",
            "Epoch 99, 1/1.\n",
            "Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.9330, 0.9473, 0.9982, 0.9463, 0.9721, 0.6364)\n",
            "Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.9330, 0.9473, 0.9982, 0.9463, 0.9721, 0.6364)\n",
            "epoch:100  0/1  loss_contrastive:-21.53245735168457  loss_label:0.050362812529384.\n",
            "repeat: 0\n",
            "mean auc, micro_f1, binary_f1, macro_f1:[0.93300972 0.94633259 0.97209562 0.63637109]\n"
          ]
        }
      ],
      "source": [
        "############################################### Training ############################################################\n",
        "\"\"\"\n",
        "This code is a Python script that trains a graph embedding model and evaluates its performance on a test dataset. \n",
        "The script starts by initializing an empty list test_results. \n",
        "Then, it enters a loop that will run once, which defines a subgraph of the main graph graph_user based on the input edge type args.edge_type. \n",
        "The script then creates a Model instance and an optimizer (Adam) for training the model. \n",
        "The script creates two datasets: dataloader_nodes for training the node embeddings and dataloader_labels for training the label embeddings. \n",
        "It also initializes a dictionary res to store the results of training.\n",
        "\n",
        "The script then enters another loop that runs for args.pretrain_epochs+args.finetune_epochs epochs. \n",
        "Within this loop, the script applies graph augmentation to the subgraph and creates batches of nodes and labels to train the model. \n",
        "It then calculates the loss on the current batch, computes gradients, and updates the model parameters. \n",
        "After every 50 epochs, the script evaluates the performance of the model on the training and test datasets and stores the results in the res dictionary.\n",
        "\n",
        "After completing the training loop, the script empties the cache and calculates the mean of the test results for each evaluation metric. \n",
        "It stores the mean values in the test_results list.\n",
        "\"\"\"\n",
        "test_results = []\n",
        "\n",
        "for m in range(1):\n",
        "    g = graph_user.edge_type_subgraph(args.edge_type)\n",
        "    pos_weight = None\n",
        "    model = Model(args).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    dataset = NodeBatch(np.arange(g.num_nodes()))\n",
        "    dataloader_nodes = torch.utils.data.DataLoader(dataset, batch_size=args.loss_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    dataset = LabelPairs(label_train)\n",
        "    dataloader_labels = torch.utils.data.DataLoader(dataset, batch_size=int(args.loss_batch_size*len(label_train)/g.num_nodes()), shuffle=True)\n",
        "    res = defaultdict(list)\n",
        "\n",
        "    for e in range(args.pretrain_epochs+args.finetune_epochs):\n",
        "        g_attr, g_stru = GraphAug(g, args)\n",
        "        cnt = 0\n",
        "\n",
        "        for nids, (pair, y) in zip(dataloader_nodes, dataloader_labels):\n",
        "            u, v = pair.T\n",
        "            y = y.to(device)\n",
        "            nids = torch.unique(torch.cat((u, v), dim=-1))\n",
        "\n",
        "            \n",
        "            embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg = model(g_attr, g_stru, nids, device)\n",
        "            loss_contrastive = model.compute_contrastive_loss(device, embs_attr_pos[nids], embs_stru_pos[nids], embs_attr_neg[nids], embs_stru_neg[nids])\n",
        "\n",
        "            y_score = model.predict_combine((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), u, v, device)\n",
        "            loss_label = model.compute_label_loss(y_score, y, pos_weight, device)\n",
        "\n",
        "            loss = args.alpha  * loss_contrastive + loss_label\n",
        "\n",
        "            print(f'epoch:{e}  {cnt}/{len(dataloader_nodes)}  loss_contrastive:{loss_contrastive}  loss_label:{loss_label}.')\n",
        "\n",
        "            opt.zero_grad() \n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            cnt += 1\n",
        "\n",
        "            \n",
        "            \n",
        "            if (e+1) % 50 ==0:\n",
        "                print(f'epoch:{e}  {cnt}/{len(dataloader_nodes)}  loss_contrastive:{loss_contrastive}  loss_label:{loss_label}.')\n",
        "                with torch.no_grad():\n",
        "                    embs, (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg) = model.inference(g, g, label_ids, device)\n",
        "                    train_auc, train_prec, train_recl, train_micro_f1, train_binary_f1, train_macro_f1 = eval_metric(embs, model, label_train, args, device)\n",
        "                    test_auc, test_prec, test_recl, test_micro_f1, test_binary_f1, test_macro_f1 = eval_metric(embs, model, label_test, args, device)\n",
        "\n",
        "                    res['train'].append([train_auc, train_prec, train_recl, train_micro_f1, train_binary_f1, train_macro_f1])\n",
        "                    res['test'].append([test_auc, test_prec, test_recl, test_micro_f1, test_binary_f1, test_macro_f1])\n",
        "                    print(f'Epoch {e}, {cnt}/{len(dataloader_nodes)}.')\n",
        "                    print(f'Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      ({train_auc:.4f}, {train_prec:.4f}, {train_recl:.4f}, {train_micro_f1:.4f}, {train_binary_f1:.4f}, {train_macro_f1:.4f})')\n",
        "                    print(f'Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      ({test_auc:.4f}, {test_prec:.4f}, {test_recl:.4f}, {test_micro_f1:.4f}, {test_binary_f1:.4f}, {test_macro_f1:.4f})')\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'repeat: {m}')\n",
        "    test_results.append([test_auc, test_micro_f1, test_binary_f1, test_macro_f1])\n",
        "    \n",
        "print(f'mean auc, micro_f1, binary_f1, macro_f1:{np.array(test_results).sum(0)/np.array(test_results).shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fu0_MFWBGBc"
      },
      "source": [
        "\n",
        "## Results\n",
        "\n",
        "> After 100 epochs on SGCL\n",
        "\n",
        "*   (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):\n",
        "* (0.9330, 0.9473, 0.9982, 0.9463, 0.9721, 0.6364)\n",
        "\n",
        "* loss_contrastive:-21.53245735168457    loss_label:0.050362812529384.\n",
        "\n",
        "* mean auc, micro_f1, binary_f1, macro_f1: [0.93300972 0.94633259 0.97209562 0.63637109]\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiPoSvOkBYLg"
      },
      "source": [
        "## For training on Logistic Regression, SVM and RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Here the embeddings of the both the nodes between the edges is calculated , after that we applied the functions like hadamard , concatenation, l1-norm, l2-norm, average function on both the embeddings and get a single embedding.\n",
        "Then we trained those embeddings along with edge labels using different models like Logistic Regression, Support Vector Machines, RandomForest and calculated \n",
        "F1 micro, F1 macro, F1 score, AUC score\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "brXCAY6HO-tH"
      },
      "outputs": [],
      "source": [
        "g = graph_user.edge_type_subgraph(args.edge_type)\n",
        "e1,e2 = GraphAug(g, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "JoxIKAoLg40T"
      },
      "outputs": [],
      "source": [
        "model = Model(args).to(device)\n",
        "embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg = model(e1, e2, nids, device)\n",
        "embs = torch.cat((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), dim=-1)\n",
        "te=model.transform(embs.cuda())\n",
        "norm_te=F.normalize(te, p=2, dim=1)\n",
        "t_np = norm_te.cpu().data.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "k9EgffCH2g1e"
      },
      "outputs": [],
      "source": [
        "df2['n1']=df2['src']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMA16Fqy8v7w",
        "outputId": "a6f0e04f-eb01-44ee-9507-70156b59b7f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:03<00:00, 7536.30it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(df2.index):\n",
        "  # print(i)\n",
        "  # print(t_np[df2['src'].loc[i]])\n",
        "  df2['n1'].loc[i]=[t_np[df2['src'].loc[i]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ybgmHmFdEZEO"
      },
      "outputs": [],
      "source": [
        "df2['n2']=df2['dst']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA52K0fF2gtp",
        "outputId": "9b5a5415-8140-47ee-9eb5-327457011e36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:03<00:00, 6993.55it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(df2.index):\n",
        "  # print(i)\n",
        "  # print(t_np[df2['src'].loc[i]])\n",
        "  df2['n2'].loc[i]=[t_np[df2['dst'].loc[i]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih9l5YjRN6RA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kziA11hAN_mN"
      },
      "source": [
        "# HADAMARD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QR4oppjIuK8",
        "outputId": "a4ea5a44-8eb6-4773-facb-4301eaed514a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:00<00:00, 26340.60it/s]\n"
          ]
        }
      ],
      "source": [
        "had_emb= []\n",
        "had_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=np.multiply(np.asarray(df2['n1'][i]), np.asarray(df2['n2'][i]))\n",
        "  had_emb.append(prod[0])\n",
        "  had_label.append(df2['label'].loc[i])\n",
        "  # print(prod)\n",
        "  # df_hadamard['had'].loc[i] = [prod]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "YZ3no2T_9iDu"
      },
      "outputs": [],
      "source": [
        "# had_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0932ypRP6yJa",
        "outputId": "04beb2ef-0197-4266-8d95-891fbf4c6bc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(had_emb)==len(had_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reRGo3bT-JDu",
        "outputId": "5fb9c286-72c4-443a-83c7-5d8f9462c4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HADAMARD:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.9353038445638694\n",
            "F1 macro: 0.48328527181458936\n",
            "F1 score: 0.9665705436291787\n",
            "AUC score: 0.5\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.9371641174038859\n",
            "F1 macro: 0.5469233650756097\n",
            "F1 score: 0.967409948542024\n",
            "AUC score: 0.5337073058796533\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.9404712691194709\n",
            "F1 macro: 0.661528744734707\n",
            "F1 score: 0.9687973997833154\n",
            "AUC score: 0.6202312322383634\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':had_emb, 'label':had_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"HADAMARD:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR1erZ_5OGHZ"
      },
      "source": [
        "# Concatenated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62rs-c5fkGhD",
        "outputId": "d94468b9-c853-434a-a480-1279c90e4364"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:00<00:00, 31673.67it/s]\n"
          ]
        }
      ],
      "source": [
        "conc_emb= []\n",
        "conc_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=prod=np.concatenate((df2['n1'][i],df2['n2'][i]))\n",
        "  conc_emb.append(prod[0])\n",
        "  conc_label.append(df2['label'].loc[i])\n",
        "  # print(prod)\n",
        "  # df_hadamard['had'].loc[i] = [prod]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTRb3qQKl6Zx",
        "outputId": "8f19a694-5e19-4763-bb2d-c82a0bed2b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.9353038445638694\n",
            "F1 macro: 0.48328527181458936\n",
            "F1 score: 0.9665705436291787\n",
            "AUC score: 0.5\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.9353038445638694\n",
            "F1 macro: 0.48328527181458936\n",
            "F1 score: 0.9665705436291787\n",
            "AUC score: 0.5\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.9361306324927656\n",
            "F1 macro: 0.6163229479860023\n",
            "F1 score: 0.966612641815235\n",
            "AUC score: 0.5837110126559935\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':conc_emb, 'label':conc_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "\n",
        "print(\"Concatenated:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2nDBDvMOeFZ"
      },
      "source": [
        "# L1 Norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDjvJBXMkzot",
        "outputId": "a3310a03-67e1-4adc-814c-4be30cb719cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:00<00:00, 31084.01it/s]\n"
          ]
        }
      ],
      "source": [
        "l1_emb= []\n",
        "l1_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=abs(np.array(df2['n1'][i])-np.array(df2['n2'][i]))\n",
        "  l1_emb.append(prod[0])\n",
        "  l1_label.append(df2['label'].loc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJYy816wmEK6",
        "outputId": "670c455a-1cb2-41f3-cc04-12010b31c326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1 Norm:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.9363373294749897\n",
            "F1 macro: 0.502289974348012\n",
            "F1 score: 0.967079948696024\n",
            "AUC score: 0.5094741672991722\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.9377842083505581\n",
            "F1 macro: 0.5526319589152238\n",
            "F1 score: 0.9677281012115364\n",
            "AUC score: 0.5370126912961362\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.9363373294749897\n",
            "F1 macro: 0.620147403111209\n",
            "F1 score: 0.9667099005620406\n",
            "AUC score: 0.5867954035973382\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':l1_emb, 'label':l1_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"L1 Norm:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAGbZ0oNOjo3"
      },
      "source": [
        "# L2 Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDyZplt7lLPD",
        "outputId": "aafbe407-b764-49fb-df74-06a4001af890"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:01<00:00, 18873.50it/s]\n"
          ]
        }
      ],
      "source": [
        "l2_emb= []\n",
        "l2_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=abs(np.array(df2['n1'][i])-np.array(df2['n2'][i]))**2\n",
        "  l2_emb.append(prod[0])\n",
        "  l2_label.append(df2['label'].loc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUXUn5BUmLLP",
        "outputId": "43da9672-9de1-4992-f29e-d125afe09c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 Norm:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.9353038445638694\n",
            "F1 macro: 0.48328527181458936\n",
            "F1 score: 0.9665705436291787\n",
            "AUC score: 0.5\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.9371641174038859\n",
            "F1 macro: 0.5392710044520629\n",
            "F1 score: 0.9674308977930147\n",
            "AUC score: 0.5292464653239899\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.9357172385283175\n",
            "F1 macro: 0.6120801828868201\n",
            "F1 score: 0.9664038025278168\n",
            "AUC score: 0.5805161244770798\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':l2_emb, 'label':l2_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"L2 Norm:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9LuU0yiOpgc"
      },
      "source": [
        "# Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVTy8cK7lLIB",
        "outputId": "12c00537-022f-4943-d6ae-86ee8580e8e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24186/24186 [00:00<00:00, 25904.08it/s]\n"
          ]
        }
      ],
      "source": [
        "avg_emb= []\n",
        "avg_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=(np.array(df2['n1'][i])+np.array(df2['n2'][i]))/2\n",
        "  avg_emb.append(prod[0])\n",
        "  avg_label.append(df2['label'].loc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhksY7rJkFt1",
        "outputId": "38abc884-58c8-4159-bd12-4c5ceda53352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average :\n",
            "Logistic Regression:\n",
            "F1 micro: 0.937577511368334\n",
            "F1 macro: 0.5397084026412622\n",
            "F1 score: 0.9676520994001714\n",
            "AUC score: 0.529467459799128\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.9353038445638694\n",
            "F1 macro: 0.4864563038476082\n",
            "F1 score: 0.96656340134601\n",
            "AUC score: 0.5014869468518879\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.9396444811905746\n",
            "F1 macro: 0.6523741092168982\n",
            "F1 score: 0.9683845820701602\n",
            "AUC score: 0.6123545090286481\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':avg_emb, 'label':avg_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"Average :\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
