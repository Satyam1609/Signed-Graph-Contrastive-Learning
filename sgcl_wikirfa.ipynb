{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VySJAWTwMAjF",
        "outputId": "338ddead-7135-44a7-d640-08ddc4f81e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d__jTszQPdTk",
        "outputId": "eaddade4-4f93-45e7-f5e1-e388318db446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dotmap in /usr/local/lib/python3.9/dist-packages (1.3.30)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/cu117/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.9/dist-packages (1.0.2+cu117)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "# !pip install dill\n",
        "!pip install dotmap\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu117/repo.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgO5hiEZPt86"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APDnKo-G_WHp"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "EKAdF8lBL5Nd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# import dill\n",
        "from dotmap import DotMap\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# pd.set_option('display.max_rows', 100)\n",
        "# pd.set_option('display.min_rows', 100)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "import dgl\n",
        "import dgl.nn\n",
        "import dgl.function as fn\n",
        "from dgl.nn import RelGraphConv\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "\n",
        "# from my_nn import GraphConv, GATConv, HeteroGraphConv\n",
        "import pickle\n",
        "import tqdm as tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "0YUN0otFNcwg"
      },
      "outputs": [],
      "source": [
        "seed = 100\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "pPKT13z0rftX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ejdNo8DrxtJ"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "iyce70RXwMzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0c6d5ee2-8cd8-471e-939a-742cc397ec99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    src  dst label\n",
              "1  9255  972     1\n",
              "2  2229  972     1\n",
              "3  4467  972     1\n",
              "4  2022  972     1\n",
              "5  6519  972     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7fc8740-849d-477c-9249-4c277f658fc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>dst</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9255</td>\n",
              "      <td>972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2229</td>\n",
              "      <td>972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4467</td>\n",
              "      <td>972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022</td>\n",
              "      <td>972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6519</td>\n",
              "      <td>972</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7fc8740-849d-477c-9249-4c277f658fc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7fc8740-849d-477c-9249-4c277f658fc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7fc8740-849d-477c-9249-4c277f658fc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/SGCL_NEW/newe_datasets/wikirfa/wikirfa.csv',header=None)\n",
        "# df.head()\n",
        "df=df.rename(columns={0: \"src\", 1: \"dst\",2:\"label\"})\n",
        "# df=df.drop(columns=0)\n",
        "df=df[1:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chJahh6Qx5Lo"
      },
      "source": [
        "# Load user and features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-xTb_w0Sd7f",
        "outputId": "df062f3e-2b8b-4147-bc51-fc579aea7135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11258, 64)\n"
          ]
        }
      ],
      "source": [
        "############################################### Load user and features ############################################################\n",
        "het_num_nodes_dict = {}\n",
        "het_node_feat_dict = {}\n",
        "het_data_dict = {}\n",
        "het_edge_feat_dict = {}\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/wikirfa/user_dict.pkl\", \"rb\") as f:\n",
        "    dict_user2id = pickle.load(f)\n",
        "het_num_nodes_dict['user'] = len(dict_user2id)\n",
        "\n",
        "# if dataset_name == 'BitCoinAlpha' or dataset_name == 'BitCoinOTC':\n",
        "user_feat = np.random.rand(len(dict_user2id), 64)\n",
        "het_node_feat_dict['user'] = user_feat\n",
        "    \n",
        "print(het_node_feat_dict['user'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJe5dKvJXQMd",
        "outputId": "647742e2-f891-4cfb-ed7e-b5083f20d6b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11258"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "len(dict_user2id.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "5ovSBsMsTAai"
      },
      "outputs": [],
      "source": [
        "# dict_user2id.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "fEh1Ma7xyfly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUIyIKo8yiit"
      },
      "source": [
        "# Load Train Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "tUbAKBdZSfyR"
      },
      "outputs": [],
      "source": [
        "############################################### Load Train Graph ############################################################\n",
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/wikirfa/g_train.pkl\", \"rb\") as f:\n",
        "    tmp_het_data_dict = pickle.load(f)\n",
        "    tmp_het_edata_dict = tmp_het_data_dict\n",
        "het_data_dict.update(tmp_het_data_dict)\n",
        "\n",
        "# for k in tmp_het_edata_dict:\n",
        "#     het_edge_feat_dict[k] = torch.from_numpy(tmp_het_edata_dict[k])\n",
        "\n",
        "graph_user = dgl.heterograph(\n",
        "    data_dict = het_data_dict,\n",
        "    num_nodes_dict = het_num_nodes_dict\n",
        ")\n",
        "for node_t in het_node_feat_dict:\n",
        "    graph_user.nodes[node_t].data['feature'] = torch.from_numpy(het_node_feat_dict[node_t]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHaVGdA8SofK",
        "outputId": "2f90ccc1-7b9e-47d3-c6c3-91cfe2e05c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "items\n",
            "(('user', 'positive', 'user'), (array([    0,     1,     2, ...,  7800,  1854, 10135]), array([  483,   483,   483, ..., 10192, 10192,  4365])))\n",
            "items\n",
            "(('user', 'negative', 'user'), (array([  119,   120,   121, ...,  2727, 10059,  7112]), array([ 483,  483,  483, ..., 9547, 9496, 1854])))\n"
          ]
        }
      ],
      "source": [
        "for i in tmp_het_data_dict.items():\n",
        "  print('items')\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8088Dnzy5m5"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Load Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "QR77uyVINf-e"
      },
      "outputs": [],
      "source": [
        "############################################### Load Labels ############################################################\n",
        "class LabelPairs(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(LabelPairs).__init__()\n",
        "        u = torch.from_numpy(df.src.values).long()\n",
        "        v = torch.from_numpy(df.dst.values).long()\n",
        "        y = torch.from_numpy(df['label'].values).double()\n",
        "        self.pairs = torch.stack((u, v), dim=0)\n",
        "        self.label = y\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.pairs[:, index], self.label[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "class NodeBatch(torch.utils.data.Dataset):\n",
        "    def __init__(self, nodes):\n",
        "        self.nodes = torch.from_numpy(nodes)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.nodes[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "zssRuFezy-lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzYJ7rXzBas"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Torch modules for graph convolutions(GCN).\"\"\"\n",
        "# pylint: disable= no-member, arguments-differ, invalid-name\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.functional import F\n",
        "\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "from dgl.nn import utils\n",
        "\n",
        "\n",
        "class GraphConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 norm='right',\n",
        "                 weight=True,\n",
        "                 bias=True,\n",
        "                 activation=None,\n",
        "                 residual=True,\n",
        "                 allow_zero_in_degree=False):\n",
        "        super(GraphConv, self).__init__()\n",
        "#         if norm not in ('none', 'both', 'right'):\n",
        "        if norm not in ('right'):\n",
        "            raise DGLError('Invalid norm value. Must be either \"none\", \"both\" or \"right\".'\n",
        "                           ' But got \"{}\".'.format(norm))\n",
        "        self._in_feats = in_feats\n",
        "        self._out_feats = out_feats\n",
        "        self._norm = norm\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        self._residual = residual\n",
        "        \n",
        "        if self._residual:\n",
        "            self.loop_weight = nn.Linear(in_feats, out_feats, bias=False)\n",
        "        \n",
        "        if weight:\n",
        "            self.weight = nn.Parameter(th.Tensor(in_feats, out_feats))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(th.Tensor(out_feats))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self._activation = activation\n",
        "\n",
        "    def reset_parameters(self):\n",
        "\n",
        "        if self.weight is not None:\n",
        "            init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, weight=None):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            if self._norm == 'both':\n",
        "                degs = graph.out_degrees().float().clamp(min=1)\n",
        "                norm = th.pow(degs, -0.5)\n",
        "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                feat_src = feat_src * norm\n",
        "\n",
        "            if self._residual:\n",
        "                loop_message = self.loop_weight(feat_dst)\n",
        "            if graph.num_edges() == 0:\n",
        "                return loop_message                \n",
        "                \n",
        "            if weight is not None:\n",
        "                if self.weight is not None:\n",
        "                    raise DGLError('External weight is provided while at the same time the'\n",
        "                                   ' module has defined its own weight parameter. Please'\n",
        "                                   ' create the module with flag weight=False.')\n",
        "            else:\n",
        "                weight = self.weight\n",
        "                \n",
        "            if self._in_feats > self._out_feats:\n",
        "                # mult W first to reduce the feature size for aggregation.\n",
        "                if weight is not None:\n",
        "                    feat_src = th.matmul(feat_src, weight)\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                 fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "            else:\n",
        "                # aggregate first then mult W\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                 fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "                if weight is not None:\n",
        "                    rst = th.matmul(rst, weight)\n",
        "\n",
        "            if self._norm != 'none':\n",
        "                degs = graph.in_degrees().float().clamp(min=1)\n",
        "                if self._norm == 'both':\n",
        "                    norm = th.pow(degs, -0.5)\n",
        "                else:\n",
        "                    norm = 1.0 / degs\n",
        "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                rst = rst * norm\n",
        "\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias\n",
        "\n",
        "            if self._activation is not None:\n",
        "                rst = self._activation(rst)\n",
        "                \n",
        "            if self._residual:\n",
        "                rst = rst + loop_message\n",
        "\n",
        "            return rst\n",
        "\n",
        "    \n",
        "    \n",
        "class GATConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False):\n",
        "        super(GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            if isinstance(feat, tuple):\n",
        "                h_src = self.feat_drop(feat[0])\n",
        "                h_dst = self.feat_drop(feat[1])\n",
        "                if not hasattr(self, 'fc_src'):\n",
        "                    self.fc_src, self.fc_dst = self.fc, self.fc\n",
        "                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "            else:\n",
        "                h_src = h_dst = self.feat_drop(feat)\n",
        "                feat_src = feat_dst = self.fc(h_src).view(\n",
        "                    -1, self._num_heads, self._out_feats)\n",
        "                if graph.is_block:\n",
        "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                    h_dst = h_src[:graph.number_of_dst_nodes()]\n",
        "            # NOTE: GAT paper uses \"first concatenation then linear projection\"\n",
        "            # to compute attention scores, while ours is \"first projection then\n",
        "            # addition\", the two approaches are mathematically equivalent:\n",
        "            # We decompose the weight vector a mentioned in the paper into\n",
        "            # [a_l || a_r], then\n",
        "            # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j\n",
        "            # Our implementation is much efficient because we do not need to\n",
        "            # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,\n",
        "            # addition could be optimized with DGL's built-in function u_add_v,\n",
        "            # which further speeds up computation and saves memory footprint.\n",
        "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
        "            graph.dstdata.update({'er': er})\n",
        "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
        "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "            e = self.leaky_relu(graph.edata.pop('e'))\n",
        "            # compute softmax\n",
        "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "            # message passing\n",
        "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                             fn.sum('m', 'ft'))\n",
        "            rst = graph.dstdata['ft']\n",
        "            # residual\n",
        "            if self.res_fc is not None:\n",
        "                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n",
        "                rst = rst + resval\n",
        "            # activation\n",
        "            if self.activation:\n",
        "                rst = self.activation(rst)\n",
        "                \n",
        "            rst = rst.view(len(rst), -1)\n",
        "                \n",
        "            return rst\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "class HeteroGraphConv(nn.Module):\n",
        "    def __init__(self, mods, dim_values, dim_query, agg_type='attn'):\n",
        "        super(HeteroGraphConv, self).__init__()\n",
        "        self.mods = nn.ModuleDict(mods)\n",
        "        # Do not break if graph has 0-in-degree nodes.\n",
        "        # Because there is no general rule to add self-loop for heterograph.\n",
        "        for _, v in self.mods.items():\n",
        "            set_allow_zero_in_degree_fn = getattr(v, 'set_allow_zero_in_degree', None)\n",
        "            if callable(set_allow_zero_in_degree_fn):\n",
        "                set_allow_zero_in_degree_fn(True)\n",
        "                \n",
        "        self.dim_values = dim_values\n",
        "        self.dim_query = dim_query\n",
        "        \n",
        "        self.attention = nn.ModuleDict()\n",
        "        for k, _ in self.mods.items():\n",
        "            self.attention[k] = nn.Sequential(nn.Linear(dim_values, dim_query), nn.Tanh(), nn.Linear(dim_query, 1, bias=False))\n",
        "        \n",
        "        self.agg_type = agg_type\n",
        "        if agg_type == 'sum':\n",
        "            self.agg_fn = th.sum\n",
        "        elif agg_type == 'max':\n",
        "            self.agg_fn = lambda inputs, dim: th.max(inputs, dim=dim)[0]\n",
        "        elif agg_type == 'min':\n",
        "            self.agg_fn = lambda inputs, dim: th.min(inputs, dim=dim)[0]\n",
        "        elif agg_type == 'stack':\n",
        "            self.agg_fn = th.stack\n",
        "        \n",
        "    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
        "        if mod_args is None:\n",
        "            mod_args = {}\n",
        "        if mod_kwargs is None:\n",
        "            mod_kwargs = {}\n",
        "        outputs = []\n",
        "        et_scores = []\n",
        "        et_count = 0\n",
        "        if isinstance(inputs, tuple) or g.is_block:\n",
        "            if isinstance(inputs, tuple):\n",
        "                src_inputs, dst_inputs = inputs\n",
        "            else:\n",
        "                src_inputs = inputs\n",
        "                dst_inputs = inputs[:g.number_of_dst_nodes()]\n",
        "\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    et_scores.append(torch.zeros(g.number_of_dst_nodes(), 1).to(g.device))\n",
        "                    outputs.append(torch.zeros(g.number_of_dst_nodes(), self.dim_values).to(g.device))\n",
        "                    continue\n",
        "                et_count += 1\n",
        "                dstdata = self.mods[etype](rel_graph, (src_inputs, dst_inputs))\n",
        "                outputs.append(dstdata)\n",
        "                et_scores.append(self.attention[etype](dstdata))\n",
        "        if len(outputs) == 0:\n",
        "            out_embs = dst_inputs\n",
        "        else:\n",
        "            et_dst_data = torch.stack(outputs, dim=0)\n",
        "            if self.agg_type == 'attn':\n",
        "                attn = torch.softmax(torch.stack(et_scores, dim=0), dim=0)\n",
        "                out_embs = (attn * et_dst_data).sum(dim=0)\n",
        "            elif self.agg_type == 'attn_sum':\n",
        "                attn = torch.softmax(torch.stack(et_scores, dim=0).mean(dim=1, keepdims=True), dim=0)\n",
        "                out_embs = (attn * et_dst_data).sum(dim=0)\n",
        "            elif self.agg_type == 'mean':\n",
        "                out_embs = torch.sum(torch.stack(et_scores, dim=0), dim=0) / et_count\n",
        "            else:\n",
        "                out_embs = self.agg_fn(et_dst_data, dim=0)\n",
        "        return out_embs, attn    "
      ],
      "metadata": {
        "id": "9Trrxx9V_nvR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "a0bYq22lNwVS"
      },
      "outputs": [],
      "source": [
        "############################################## Define Model ############################################################\n",
        "\"\"\"Torch modules for graph convolutions(GCN).\"\"\"\n",
        "# pylint: disable= no-member, arguments-differ, invalid-name\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.functional import F\n",
        "\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import edge_softmax\n",
        "from dgl.nn import utils\n",
        "\n",
        "\n",
        "'''\n",
        "Define Hetero Layer\n",
        "'''\n",
        "class HetAttn(nn.Module):\n",
        "    def __init__(self, args, etypes):\n",
        "        super(HetAttn, self).__init__()\n",
        "        self.args = DotMap(args.toDict())\n",
        "        args = self.args\n",
        "        self.etypes = etypes\n",
        "        self.feature_trans = nn.Linear(args.dim_features, args.dim_hiddens, bias=False)\n",
        "        self.convs = nn.ModuleList()\n",
        "        \n",
        "        if self.args.conv_type == 'gcn':\n",
        "            for _ in range(args.conv_depth - 1):\n",
        "                self.convs.append(HeteroGraphConv({rel: GraphConv(args.dim_hiddens, args.dim_hiddens, allow_zero_in_degree=True, residual=args.residual)\n",
        "                                                   for rel in self.etypes},\n",
        "                                  agg_type=args.het_agg_type, dim_values=args.dim_hiddens, dim_query=args.dim_query)) \n",
        "            self.convs.append(HeteroGraphConv({rel: GraphConv(args.dim_hiddens, args.dim_embs, allow_zero_in_degree=True, residual=args.residual) \n",
        "                                               for rel in self.etypes},\n",
        "                              agg_type=args.het_agg_type, dim_values=args.dim_embs, dim_query=args.dim_query))\n",
        "        elif self.args.conv_type == 'gat':\n",
        "            for _ in range(args.conv_depth - 1):\n",
        "                self.convs.append(HeteroGraphConv({rel: GATConv(args.dim_hiddens, args.dim_hiddens // args.num_heads, args.num_heads, allow_zero_in_degree=True, residual=args.residual)\n",
        "                                                   for rel in self.etypes},\n",
        "                                  agg_type=args.het_agg_type, dim_values=args.dim_hiddens, dim_query=args.dim_query)) \n",
        "            self.convs.append(HeteroGraphConv({rel: GATConv(args.dim_hiddens, args.dim_embs // args.num_heads, args.num_heads, allow_zero_in_degree=True, residual=args.residual) \n",
        "                                               for rel in self.etypes},\n",
        "                          agg_type=args.het_agg_type, dim_values=args.dim_embs, dim_query=args.dim_query))\n",
        "        \n",
        "        self.concat_weight = nn.Linear((args.conv_depth + 1) * args.dim_embs, args.dim_embs, bias=False)\n",
        "\n",
        "        if self.args.conv_depth == 1:\n",
        "            self.sampler_nodes = [5]\n",
        "            self.sampler_inference = [10]\n",
        "        elif self.args.conv_depth == 2:\n",
        "            self.sampler_nodes = [10, 20]\n",
        "            self.sampler_inference = [10 , 20]\n",
        "        else:\n",
        "            raise\n",
        "#         if self.args.conv_depth == 3:\n",
        "#             self.sampler_nodes = [5, 10, 10]\n",
        "#             self.sampler_inference = [10, 10, 10]\n",
        "            \n",
        "        nn.init.xavier_uniform_(self.feature_trans.weight)\n",
        "        \n",
        "        \n",
        "    def calc_from_loader(self, loader, x, device):\n",
        "        y = torch.zeros(len(x), self.args.dim_embs)\n",
        "        attn_res = torch.zeros(len(x), len(self.etypes))\n",
        "        \n",
        "        def calc_from_blocks(blocks, conv_idx, x, device):\n",
        "            input_nodes, output_nodes = blocks[0].srcdata[dgl.NID], blocks[0].dstdata[dgl.NID]\n",
        "            h = x[input_nodes].to(device)\n",
        "            h = torch.tanh(self.feature_trans(h))\n",
        "            for b, idx in zip(blocks, conv_idx):\n",
        "                b = b.to(device)\n",
        "                h, attn = self.convs[idx](b, h)\n",
        "            return h, attn\n",
        "        \n",
        "        for input_nodes, output_nodes, blocks in loader:\n",
        "            \n",
        "            h0 = x[output_nodes].to(device)\n",
        "            h0 = torch.tanh(self.feature_trans(h0))\n",
        "            emb_ulti = [h0]\n",
        "            if self.args.conv_depth == 1:\n",
        "                h1, attn = calc_from_blocks(blocks, [0], x, device)\n",
        "                emb_ulti.append(h1)\n",
        "            if self.args.conv_depth ==2:\n",
        "                h1, _ = calc_from_blocks(blocks[1:], [1], x, device)\n",
        "                h2, attn = calc_from_blocks(blocks, [0, 1], x, device)\n",
        "                emb_ulti.extend([h1, h2])\n",
        "            \n",
        "            y[output_nodes] = self.concat_weight(torch.cat(emb_ulti, dim=-1)).cpu()\n",
        "            attn_res[output_nodes] = attn.squeeze(dim=-1).transpose(0, 1).cpu()\n",
        "        return y, attn_res\n",
        "\n",
        "    def forward(self, g, x, nids, device):\n",
        "        dataloader = dgl.dataloading.DataLoader(g, nids,\n",
        "                                                    dgl.dataloading.MultiLayerNeighborSampler(self.sampler_nodes),\n",
        "                                                    batch_size=self.args.sampling_batch_size,\n",
        "                                                    num_workers=self.args.num_workers,\n",
        "                                                    shuffle=True,\n",
        "                                                    drop_last=False)\n",
        "        y, attn_res = self.calc_from_loader(dataloader, x, device)\n",
        "        return y, attn_res\n",
        "    \n",
        "    \n",
        "    def inference(self, g, x, nids, device):\n",
        "#         dataloader = dgl.dataloading.NodeDataLoader(g, nids,\n",
        "#                                                     dgl.dataloading.MultiLayerFullNeighborSampler(len(self.sampler_nodes)),\n",
        "#                                                     batch_size=self.args.inference_batch_size,\n",
        "#                                                     num_workers=self.args.num_workers,\n",
        "#                                                     shuffle=True,\n",
        "#                                                     drop_last=False)\n",
        "        dataloader = dgl.dataloading.DataLoader(g, nids,\n",
        "                                                    dgl.dataloading.MultiLayerNeighborSampler(self.sampler_inference),\n",
        "                                                    batch_size=self.args.sampling_batch_size,\n",
        "                                                    num_workers=self.args.num_workers,\n",
        "                                                    shuffle=True,\n",
        "                                                    drop_last=False)\n",
        "        y, attn_res = self.calc_from_loader(dataloader, x, device)\n",
        "        return y, attn_res\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "ZQcYZ-V_OSeR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define Whole Model\n",
        "'''\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        if args.sign_conv == 'sign':\n",
        "            self.pos_emb_model = HetAttn(args, args.pos_edge_type)\n",
        "            self.neg_emb_model = HetAttn(args, args.neg_edge_type)\n",
        "        elif args.sign_conv == 'common':\n",
        "            self.emb_model = HetAttn(args, args.edge_type)\n",
        "        self.args = args\n",
        "        self.link_predictor = ScorePredictor(args, dim_embs = args.dim_embs)\n",
        "        \n",
        "        self.combine_type = args.combine_type\n",
        "        \n",
        "        if self.args.sign_aggre!='both':\n",
        "            transform_type = 2\n",
        "        elif self.args.sign_aggre == 'both' or self.args.sign_conv == 'common':\n",
        "            transform_type = 4\n",
        "        \n",
        "        if self.combine_type == 'concat':\n",
        "            self.transform = nn.Sequential(nn.Linear(transform_type*args.dim_embs, args.dim_embs))   #transform\n",
        "            self.link_predictor = ScorePredictor(args, dim_embs = args.dim_embs)\n",
        "        elif self.combine_type == 'attn':\n",
        "            self.attention = nn.Sequential(nn.Linear(args.dim_embs, args.dim_query), nn.Tanh(), nn.Linear(args.dim_query, 1, bias=False))\n",
        "            self.link_predictor = ScorePredictor(args, dim_embs=args.dim_embs)\n",
        "        \n",
        "    def forward(self, g_attr, g_stru, nids, device):\n",
        "#         nids = torch.unique(torch.cat((uids, vids), dim=-1))\n",
        "#         embs_pos, embs_neg = self.emb_model(g, x, nids, device)\n",
        "#         score = self.predict_combine((embs_pos, embs_neg), uids, vids, device)\n",
        "        if self.args.sign_conv == 'common':\n",
        "            embs_attr_pos, _ = self.emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, _ = self.emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, _ = self.emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, _ = self.emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            return embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg\n",
        "        elif self.args.sign_conv == 'sign':\n",
        "            embs_attr_pos, _ = self.pos_emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, _ = self.neg_emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, _ = self.pos_emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, _ = self.neg_emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            return embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg\n",
        "    \n",
        "    def inference(self, g_attr, g_stru, nids, device):\n",
        "        if self.args.sign_conv == 'common':\n",
        "            embs_attr_pos, attn_attr_pos = self.emb_model(g_attr, g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, attn_attr_neg = self.emb_model(g_attr, g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, attn_stru_pos = self.emb_model(g_stru, g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, attn_stru_neg = self.emb_model(g_stru, g_stru.ndata['feature'], nids, device)\n",
        "            return (embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg), (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg)\n",
        "        elif self.args.sign_conv == 'sign':\n",
        "            embs_attr_pos, attn_attr_pos = self.pos_emb_model(g_attr.edge_type_subgraph(self.args.pos_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_attr_neg, attn_attr_neg = self.neg_emb_model(g_attr.edge_type_subgraph(self.args.neg_edge_type), g_attr.ndata['feature'], nids, device)\n",
        "            embs_stru_pos, attn_stru_pos = self.pos_emb_model(g_stru.edge_type_subgraph(self.args.pos_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            embs_stru_neg, attn_stru_neg = self.neg_emb_model(g_stru.edge_type_subgraph(self.args.neg_edge_type), g_stru.ndata['feature'], nids, device)\n",
        "            print()\n",
        "            return (embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg), (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg)\n",
        "\n",
        "        \n",
        "    def compute_contrastive_loss(self, device, embs_attr_pos, embs_stru_pos, embs_attr_neg=None, embs_stru_neg=None):\n",
        "        nodes_num = embs_attr_pos.shape[0]\n",
        "        feature_size = embs_attr_pos.shape[1]\n",
        "        \n",
        "        embs_attr_pos = embs_attr_pos.to(device)\n",
        "        embs_stru_pos = embs_stru_pos.to(device)\n",
        "        normalized_embs_attr_pos = F.normalize(embs_attr_pos, p=2, dim=1)\n",
        "        normalized_embs_stru_pos = F.normalize(embs_stru_pos, p=2, dim=1)\n",
        "        if embs_attr_neg!=None and embs_stru_neg!=None:\n",
        "            embs_attr_neg = embs_attr_neg.to(device)\n",
        "            embs_stru_neg = embs_stru_neg.to(device)\n",
        "            normalized_embs_attr_neg = F.normalize(embs_attr_neg, p=2, dim=1)\n",
        "            normalized_embs_stru_neg = F.normalize(embs_stru_neg, p=2, dim=1)\n",
        "        \n",
        "        \n",
        "        def inter_contrastive(embs_attr, embs_stru):\n",
        "            pos = torch.exp(torch.div(torch.bmm(embs_attr.view(nodes_num, 1, feature_size), embs_stru.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            \n",
        "            def generate_neg_score(embs_1, embs_2):\n",
        "                neg_similarity = torch.mm(embs_1.view(nodes_num, feature_size), embs_2.transpose(0,1))\n",
        "                neg_similarity[np.arange(nodes_num),np.arange(nodes_num)] = 0\n",
        "                return torch.sum(torch.exp(torch.div( neg_similarity  , self.args.tao)) , dim=1)\n",
        "            \n",
        "            neg = generate_neg_score(embs_attr, embs_stru)\n",
        "\n",
        "            return torch.mean(- (torch.log(torch.div(pos, neg))))\n",
        "        \n",
        "        def intra_contrastive(self_embs, embs_attr_pos, embs_attr_neg, embs_stru_pos, embs_stru_neg):\n",
        "            pos_score_1 = torch.exp(torch.div(torch.bmm(self_embs.view(nodes_num, 1, feature_size), embs_attr_pos.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            pos_score_2 = torch.exp(torch.div(torch.bmm(self_embs.view(nodes_num, 1, feature_size), embs_stru_pos.view(nodes_num, feature_size, 1)), self.args.tao))\n",
        "            pos = pos_score_1 + pos_score_2\n",
        "            def generate_neg_score(pos_embs, neg_embs_1, neg_embs_2):\n",
        "                neg_score_1 = torch.bmm(pos_embs.view(nodes_num, 1, feature_size), neg_embs_1.view(nodes_num, feature_size, 1))\n",
        "                neg_score_2 = torch.bmm(pos_embs.view(nodes_num, 1, feature_size), neg_embs_2.view(nodes_num, feature_size, 1))\n",
        "                return torch.exp(torch.div(neg_score_1, self.args.tao)) + torch.exp(torch.div(neg_score_2, self.args.tao))\n",
        "            neg = generate_neg_score(self_embs, embs_attr_neg, embs_stru_neg)\n",
        "            return torch.mean(- torch.log(torch.div(pos, neg)) )\n",
        "            \n",
        "\n",
        "        inter_pos = inter_contrastive(normalized_embs_attr_pos, normalized_embs_stru_pos)\n",
        "        inter_neg = inter_contrastive(normalized_embs_attr_neg, normalized_embs_stru_neg)\n",
        "        \n",
        "        embs = torch.cat((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), dim=-1)\n",
        "        # print(\"#####################################\")\n",
        "        # print(\"embs_attr_pos\")\n",
        "        # print(embs_attr_pos.size())\n",
        "        # print(\"embs_stru_pos\")\n",
        "        # print(embs_stru_pos.size())\n",
        "        # print(\"embs_attr_neg\")\n",
        "        # print(embs_attr_neg.size())\n",
        "        # print(\"embs_stru_neg\")\n",
        "        # print(embs_stru_neg.size())\n",
        "   \n",
        "        # print(\"concatenated emb of ultimate representation\")\n",
        "        # print(embs.size())\n",
        "        self_embs = self.transform(embs)\n",
        "        # print(\"transformed emb of ultimate representation \")\n",
        "        # print(self_embs.size())\n",
        "        normalized_self_embs = F.normalize(self_embs, p=2, dim=1)\n",
        "        # print(\"normalized emb of ultimate representation \")\n",
        "        # print(normalized_self_embs.size())\n",
        "\n",
        "        \n",
        "        intra = intra_contrastive(normalized_self_embs, normalized_embs_attr_pos, normalized_embs_attr_neg, normalized_embs_stru_pos, normalized_embs_stru_neg)\n",
        "        # print(f'inter_pos:{inter_pos}  inter_neg:{inter_neg}  intra:{intra}')\n",
        "        if self.args.contrast_type == 'pos':\n",
        "            return inter_pos\n",
        "        elif self.args.contrast_type == 'neg':\n",
        "            return inter_neg\n",
        "        elif self.args.contrast_type == 'intra':\n",
        "            return intra\n",
        "        elif self.args.contrast_type == 'inter':\n",
        "            return inter_pos + inter_neg\n",
        "        elif self.args.contrast_type == 'all':\n",
        "            return (1-self.args.beta) * (inter_pos + inter_neg) + self.args.beta * intra\n",
        "            \n",
        "\n",
        "        \n",
        "    \n",
        "    def compute_label_loss(self, score, y_label, pos_weight, device):\n",
        "        pos_weight = torch.tensor([(y_label==0).sum().item()/(y_label==1).sum().item()]*y_label.shape[0]).to(device)\n",
        "        return F.binary_cross_entropy_with_logits(score, y_label, pos_weight=pos_weight)\n",
        "    \n",
        "        \n",
        "        \n",
        "    def predict_combine(self, embs, uids, vids, device):\n",
        "        u_embs = self.combine(embs, uids, device)\n",
        "        v_embs = self.combine(embs, vids, device)\n",
        "        score = self.link_predictor(u_embs, v_embs)\n",
        "        return score\n",
        "    \n",
        "    def compute_attention(self, embs):\n",
        "        attn = self.attention(embs).softmax(dim=0)\n",
        "        return attn\n",
        "    \n",
        "    def combine(self, embs, nids, device):\n",
        "        if self.args.sign_conv == 'sign':\n",
        "            if self.args.sign_aggre == 'pos':\n",
        "                embs = (embs[0],embs[1])\n",
        "            elif self.args.sign_aggre == 'neg':\n",
        "                embs = (embs[2],embs[3])\n",
        "            \n",
        "        if self.combine_type == 'concat':\n",
        "            embs = torch.cat(embs, dim=-1)\n",
        "            sub_embs = embs[nids].to(device)\n",
        "            out_embs = self.transform(sub_embs)\n",
        "            return out_embs                          #output embs\n",
        "        elif self.combine_type == 'attn':\n",
        "            embs = torch.stack(embs, dim=0)\n",
        "            sub_embs = embs[:,nids].to(device)\n",
        "            attn = self.compute_attention(sub_embs)\n",
        "            # attn: (2,n,1)   sub_embs: (2,n,feature)\n",
        "            out_embs = (attn*sub_embs).sum(dim=0)\n",
        "            return out_embs\n",
        "        elif self.combine_type == 'mean':\n",
        "            embs = torch.stack(embs, dim=0).mean(dim=0)\n",
        "            sub_embs = embs[nids].to(device)\n",
        "            return sub_embs\n",
        "        elif self.combine_type == 'pos':\n",
        "            sub_embs = embs[0][nids].to(device)\n",
        "            return sub_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "OPyI8dHHzICO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scpN3BUzxsd"
      },
      "source": [
        "# Graph Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "sEjGcQ2SOzjM"
      },
      "outputs": [],
      "source": [
        "############################################### Graph Augmentation ############################################################\n",
        "\n",
        "def generate_mask(mask_ratio, row, column):\n",
        "    # 1 -- leave   0 -- drop\n",
        "    arr_mask_ratio = np.random.uniform(0,1,size=(row, column))\n",
        "    arr_mask = np.ma.masked_array(arr_mask_ratio, mask=(arr_mask_ratio<mask_ratio)).filled(0)\n",
        "    arr_mask = np.ma.masked_array(arr_mask, mask=(arr_mask>=mask_ratio)).filled(1)\n",
        "    return arr_mask\n",
        "\n",
        "def generate_attr_graph(g, args):\n",
        "    # generate noise g_attr\n",
        "    feature = g.ndata['feature']\n",
        "    attr_noise = np.random.normal(loc=0, scale=0.1, size=(feature.shape[0], feature.shape[1]))\n",
        "    attr_mask = generate_mask(args.mask_ratio, row=feature.shape[0], column=feature.shape[1])\n",
        "    noise_feature = feature*attr_mask + (1-attr_mask) * attr_noise\n",
        "    \n",
        "    g_attr = g\n",
        "    g_attr.ndata['feature'] = noise_feature.float()\n",
        "    return g_attr\n",
        "\n",
        "def generate_stru_graph(g, args):\n",
        "    # generate noise g_stru by deleting links\n",
        "    g_stru = g\n",
        "\n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "        \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        # shape: (e, 2)\n",
        "        df = np.array([etype_edges[0].numpy(), etype_edges[1].numpy()]).transpose()\n",
        "        \n",
        "        # delete edges\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "\n",
        "        # add an equal number of edges\n",
        "        add_row = []\n",
        "        add_column = []\n",
        "        index = 0\n",
        "        while index < len(drop_eids):\n",
        "            row_sample = np.random.randint(g.num_nodes())\n",
        "            column_sample = np.random.randint(g.num_nodes())\n",
        "            if (df==[row_sample, column_sample]).all(1).any() == False:\n",
        "                index += 1\n",
        "                add_row.append(row_sample)\n",
        "                add_column.append(column_sample)\n",
        "        g_stru = dgl.add_edges(g_stru, add_row, add_column, etype=etype)\n",
        "\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\n",
        "\n",
        "def generate_stru_sign_graph(g, args):\n",
        "    # generate noise g_stru by exchanging some pos/neg links\n",
        "    g_stru = g\n",
        "    \n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "    \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        \n",
        "        # delete edges\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "        \n",
        "        # add_edges\n",
        "        if etype in args.pos_edge_type:\n",
        "            g_stru = dgl.add_edges(g_stru, etype_edges[0][drop_eids], etype_edges[1][drop_eids] , etype=random.choice(args.neg_edge_type))\n",
        "        elif etype in args.neg_edge_type:\n",
        "            g_stru = dgl.add_edges(g_stru, etype_edges[0][drop_eids], etype_edges[1][drop_eids] , etype=random.choice(args.pos_edge_type))\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\n",
        "def generate_stru_status_graph(g, args):\n",
        "    g_stru = g\n",
        "    \n",
        "    if args.drop_type == 'both':\n",
        "        edge_types = args.edge_type\n",
        "    elif args.drop_type == 'pos':\n",
        "        edge_types = args.pos_edge_type\n",
        "    elif args.drop_type == 'neg':\n",
        "        edge_types = args.neg_edge_type\n",
        "    \n",
        "    for etype in edge_types:\n",
        "        etype_edges = g.edges(etype=etype)\n",
        "        edge_mask = generate_mask(args.mask_ratio, row=1, column=len(etype_edges[0]))\n",
        "        \n",
        "        # delete edges\n",
        "        drop_eids = torch.arange(0,len(etype_edges[0]))[edge_mask==0]\n",
        "        g_stru = dgl.remove_edges(g_stru, drop_eids, etype=etype)\n",
        "        \n",
        "        # add reverse_edges\n",
        "        g_stru = dgl.add_edges(g_stru, etype_edges[1][drop_eids], etype_edges[0][drop_eids], etype=etype)\n",
        "    g_stru.ndata['feature'] = g_stru.ndata['feature'].float()\n",
        "    return g_stru\n",
        "\n",
        "def GraphAug(g, args):\n",
        "    if args.augment == 'delete':     #for connectivity perturbation\n",
        "        g_attr = generate_stru_graph(g, args)\n",
        "        g_stru = generate_stru_graph(g, args)\n",
        "    elif args.augment == 'change':          #for sign perturbation\n",
        "        g_attr = generate_stru_sign_graph(g, args)\n",
        "        g_stru = generate_stru_sign_graph(g, args)\n",
        "    elif args.augment == 'reverse':\n",
        "        g_attr = generate_stru_status_graph(g, args)\n",
        "        g_stru = generate_stru_status_graph(g, args)\n",
        "    elif args.augment == 'composite':\n",
        "        g_attr = generate_stru_sign_graph(g, args)\n",
        "        g_stru = generate_stru_graph(g, args)\n",
        "    return g_attr, g_stru\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVXqMuazI8e"
      },
      "source": [
        "# Define Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "TSSanEGOOnI3"
      },
      "outputs": [],
      "source": [
        "############################################### Define Predictor ############################################################\n",
        "class ScorePredictor(nn.Module):\n",
        "    def __init__(self, args, **params):\n",
        "        super().__init__()\n",
        "        self.args = DotMap(args.toDict())\n",
        "        for k,v in params.items():\n",
        "            self.args[k] = v\n",
        "        \n",
        "        if self.args.predictor == 'dot':\n",
        "            pass\n",
        "        elif self.args.predictor == '1-linear':\n",
        "            self.predictor = nn.Linear(self.args.dim_embs*2, 1)\n",
        "        elif self.args.predictor == '2-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1))\n",
        "        elif self.args.predictor == '3-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1)\n",
        "                                         )\n",
        "        elif self.args.predictor == '4-linear':\n",
        "            self.predictor = nn.Sequential(nn.Linear(self.args.dim_embs*2, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, self.args.dim_embs),\n",
        "                                          nn.LeakyReLU(),\n",
        "                                          nn.Linear(self.args.dim_embs, 1)\n",
        "                                         )\n",
        "        self.reset_parameters()\n",
        "            \n",
        "    def reset_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, u_e, u_v):\n",
        "        if self.args.predictor == 'dot':\n",
        "            score = u_e.mul(u_v).sum(dim=-1)\n",
        "        else:\n",
        "            x = torch.cat([u_e, u_v], dim=-1)\n",
        "            score = self.predictor(x).flatten()\n",
        "        return score\n",
        "\n",
        "def eval_model(embs, model, df, batched, args, device):\n",
        "    if batched:\n",
        "        dataset = LabelPairs(df)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.loss_batch_size, num_workers=args.num_workers, shuffle=True)\n",
        "        y_pre_list = []\n",
        "        y_true_list = []\n",
        "        for pair, y in dataloader:\n",
        "            uids, vids = pair.T\n",
        "            score  = model.predict_combine(embs, uids, vids, device)\n",
        "            y_pre_list.append(torch.sigmoid(score))\n",
        "            y_true_list.append(y)\n",
        "        y_pre = torch.cat(y_pre_list, dim=-1).cpu().numpy()\n",
        "        y_true = torch.cat(y_true_list, dim=-1).cpu().numpy()\n",
        "    else:\n",
        "        uids = torch.from_numpy(df.src.values).long()\n",
        "        vids = torch.from_numpy(df.dst.values).long()\n",
        "        score  = model.predict_combine(embs, uids, vids, device)\n",
        "        y_pre = torch.sigmoid(score).cpu().numpy()\n",
        "        y_true = df['label'].values\n",
        "    return y_true, y_pre\n",
        "    \n",
        "def eval_metric(embs, model, df, args, device, threshold=0.05):\n",
        "\t# change threshold according to different datasets\n",
        "\t# 0.05 for Alpha, 0.1 for OTC\n",
        "    y_true, y_pre = eval_model(embs, model, df, args.eval_batched, args, device)\n",
        "    y = (y_pre > threshold)\n",
        "    auc = metrics.roc_auc_score(y_true, y_pre)\n",
        "    prec = metrics.precision_score(y_true, y)\n",
        "    recl = metrics.recall_score(y_true, y)\n",
        "    binary_f1 = metrics.f1_score(y_true, y, average='binary')\n",
        "    micro_f1 = metrics.f1_score(y_true, y, average='micro')\n",
        "    macro_f1 = metrics.f1_score(y_true, y, average='macro')\n",
        "    \n",
        "    \n",
        "    return auc, prec, recl, micro_f1, binary_f1, macro_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "LCt-jdwfzMK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ygezuIzYHu"
      },
      "source": [
        "# Training Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "uXzmCBq4OsHG"
      },
      "outputs": [],
      "source": [
        "############################################### Training Parameter Setting ############################################################\n",
        "\n",
        "args = DotMap()\n",
        "args.num_nodes = graph_user.num_nodes()\n",
        "\n",
        "args.pos_edge_type = ['positive']\n",
        "args.neg_edge_type = ['negative']\n",
        "args.edge_type = args.pos_edge_type+args.neg_edge_type\n",
        "args.num_edge_types = len(args.edge_type)\n",
        "args.dim_features = graph_user.nodes['user'].data['feature'].shape[1]\n",
        "args.dim_hiddens = 128\n",
        "args.dim_embs = 128\n",
        "\n",
        "args.learning_rate = 0.01\n",
        "\n",
        "args.conv_depth = 2\n",
        "args.loss_batch_size = 102400             # to calculated loss\n",
        "\n",
        "#args.inference_batch_size = 128       # the batch size for inferencing all/batched nodes embeddings\n",
        "args.sampling_batch_size = 128\n",
        "args.residual = False\n",
        "args.num_heads = 8\n",
        "args.dropout = 0\n",
        "\n",
        "# active_tag walktogether_tag  friend_tag  playagain_tag \n",
        "# label\n",
        "args.label = 'label'  \n",
        "args.conv_type = 'gat'\n",
        "args.het_agg_type = 'attn' # multiplex aggregation\n",
        "args.dim_query = 128\n",
        "args.predictor = '2-linear'\n",
        "# concat / mean / attn / pos\n",
        "args.combine_type = 'concat'\n",
        "\n",
        "# sign / common\n",
        "args.sign_conv = 'sign'\n",
        "# pos / neg / both\n",
        "args.sign_aggre = 'both'\n",
        "# pos / neg / intra / inter / all\n",
        "args.contrast_type = 'all'\n",
        "# delete / change / reverse / composite\n",
        "args.augment = 'change'\n",
        "\n",
        "#args.contrastive = True\n",
        "args.mask_ratio = 0.1\n",
        "args.tao = 0.05\n",
        "args.alpha = 1e-4\n",
        "args.beta = 0.8\n",
        "args.pos_gamma = 1\n",
        "args.neg_gamma = 1\n",
        "\n",
        "args.gpu = 0\n",
        "args.num_workers = 0\n",
        "args.verbose = 1\n",
        "args.pretrain_epochs = 101\n",
        "args.finetune_epochs = 0\n",
        "# both / pos / neg\n",
        "args.drop_type = 'both'\n",
        "\n",
        "# 2-layer 20\n",
        "\n",
        "device = torch.device(f'cuda:{args.gpu}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "0FesTBfSnpsR"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "mWnquZhYnv8d"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/wikirfa/label_train.pkl\", \"rb\") as f:\n",
        "    label_train = pickle.load(f)\n",
        "    label_test =  label_train\n",
        "    \n",
        "label_ids = np.unique(np.concatenate((label_train.src, label_train.dst, label_test.src, label_test.dst)))"
      ],
      "metadata": {
        "id": "jokLF32knQzh"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_train.to_csv(\"/content/drive/MyDrive/SGCL_NEW/newe_datasets/wikirfa/wikirfa_train.csv\")"
      ],
      "metadata": {
        "id": "owJm5UUv0V6Z"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Gt-0BeCLIeyr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "0551537d-2668-4dc7-890d-0b15944fcc3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          src    dst  label\n",
              "1           0    483      1\n",
              "2           1    483      1\n",
              "3           2    483      1\n",
              "4           3    483      1\n",
              "5           4    483      1\n",
              "...       ...    ...    ...\n",
              "198271   7118  10276      1\n",
              "198272   5993  10192      1\n",
              "198273   7800  10192      1\n",
              "198274   1854  10192      1\n",
              "198275  10135   4365      1\n",
              "\n",
              "[185627 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18fecb38-c4c2-4928-9483-1245e0969983\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>dst</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198271</th>\n",
              "      <td>7118</td>\n",
              "      <td>10276</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198272</th>\n",
              "      <td>5993</td>\n",
              "      <td>10192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198273</th>\n",
              "      <td>7800</td>\n",
              "      <td>10192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198274</th>\n",
              "      <td>1854</td>\n",
              "      <td>10192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198275</th>\n",
              "      <td>10135</td>\n",
              "      <td>4365</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>185627 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18fecb38-c4c2-4928-9483-1245e0969983')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18fecb38-c4c2-4928-9483-1245e0969983 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18fecb38-c4c2-4928-9483-1245e0969983');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "df2=label_train\n",
        "type(label_train)\n",
        "label_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZEW9Ykmz3gx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zMdZkdWg6Ns",
        "outputId": "d116a752-999f-4433-f9ec-5734a689cc32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0  0/1  loss_contrastive:2.2158021926879883  loss_label:0.30826725378294023.\n",
            "epoch:1  0/1  loss_contrastive:-3.5175602436065674  loss_label:0.3090233878630391.\n",
            "epoch:2  0/1  loss_contrastive:-7.753384590148926  loss_label:0.3068678356165425.\n",
            "epoch:3  0/1  loss_contrastive:-11.17943286895752  loss_label:0.3069641265979183.\n",
            "epoch:4  0/1  loss_contrastive:-13.745476722717285  loss_label:0.30639816831282435.\n",
            "epoch:5  0/1  loss_contrastive:-15.54903793334961  loss_label:0.3062454512336489.\n",
            "epoch:6  0/1  loss_contrastive:-17.0677547454834  loss_label:0.30540468821156297.\n",
            "epoch:7  0/1  loss_contrastive:-18.179988861083984  loss_label:0.30498280227460134.\n",
            "epoch:8  0/1  loss_contrastive:-19.017072677612305  loss_label:0.30384772618096906.\n",
            "epoch:9  0/1  loss_contrastive:-19.615236282348633  loss_label:0.30303514116797103.\n",
            "epoch:10  0/1  loss_contrastive:-20.063982009887695  loss_label:0.30305902531374107.\n",
            "epoch:11  0/1  loss_contrastive:-20.478981018066406  loss_label:0.3025918220857549.\n",
            "epoch:12  0/1  loss_contrastive:-20.798870086669922  loss_label:0.3002988310268128.\n",
            "epoch:13  0/1  loss_contrastive:-21.08965301513672  loss_label:0.3004281556180504.\n",
            "epoch:14  0/1  loss_contrastive:-21.403385162353516  loss_label:0.3013532884479194.\n",
            "epoch:15  0/1  loss_contrastive:-21.532487869262695  loss_label:0.2983070137107999.\n",
            "epoch:16  0/1  loss_contrastive:-21.68494415283203  loss_label:0.29736216178976715.\n",
            "epoch:17  0/1  loss_contrastive:-21.878843307495117  loss_label:0.2982059790739617.\n",
            "epoch:18  0/1  loss_contrastive:-21.924165725708008  loss_label:0.2961390743931353.\n",
            "epoch:19  0/1  loss_contrastive:-21.89000701904297  loss_label:0.29651652556871216.\n",
            "epoch:20  0/1  loss_contrastive:-22.088363647460938  loss_label:0.29644908611003984.\n",
            "epoch:21  0/1  loss_contrastive:-22.10669708251953  loss_label:0.2943215299209472.\n",
            "epoch:22  0/1  loss_contrastive:-22.003189086914062  loss_label:0.29517395039032734.\n",
            "epoch:23  0/1  loss_contrastive:-22.17706298828125  loss_label:0.2942539905163563.\n",
            "epoch:24  0/1  loss_contrastive:-22.184568405151367  loss_label:0.29217466363216743.\n",
            "epoch:25  0/1  loss_contrastive:-22.137147903442383  loss_label:0.293737539430568.\n",
            "epoch:26  0/1  loss_contrastive:-22.304067611694336  loss_label:0.2927949048780724.\n",
            "epoch:27  0/1  loss_contrastive:-22.32262420654297  loss_label:0.29100021042832724.\n",
            "epoch:28  0/1  loss_contrastive:-22.27895164489746  loss_label:0.29158117187522625.\n",
            "epoch:29  0/1  loss_contrastive:-22.41431999206543  loss_label:0.2917867380581794.\n",
            "epoch:30  0/1  loss_contrastive:-22.36842155456543  loss_label:0.2907310726800568.\n",
            "epoch:31  0/1  loss_contrastive:-22.39156150817871  loss_label:0.28971214327061096.\n",
            "epoch:32  0/1  loss_contrastive:-22.455442428588867  loss_label:0.289205068583002.\n",
            "epoch:33  0/1  loss_contrastive:-22.33767318725586  loss_label:0.29629055360796624.\n",
            "epoch:34  0/1  loss_contrastive:-22.589908599853516  loss_label:0.30996511965391893.\n",
            "epoch:35  0/1  loss_contrastive:-22.36789894104004  loss_label:0.2960682117611337.\n",
            "epoch:36  0/1  loss_contrastive:-22.168231964111328  loss_label:0.3038178089139156.\n",
            "epoch:37  0/1  loss_contrastive:-22.284032821655273  loss_label:0.291526944299984.\n",
            "epoch:38  0/1  loss_contrastive:-22.261568069458008  loss_label:0.30019789578126493.\n",
            "epoch:39  0/1  loss_contrastive:-22.097827911376953  loss_label:0.2982047500730794.\n",
            "epoch:40  0/1  loss_contrastive:-21.79789161682129  loss_label:0.29256716200848093.\n",
            "epoch:41  0/1  loss_contrastive:-21.471961975097656  loss_label:0.29567578955352414.\n",
            "epoch:42  0/1  loss_contrastive:-21.27489471435547  loss_label:0.29582625559708403.\n",
            "epoch:43  0/1  loss_contrastive:-21.22640609741211  loss_label:0.29252310314715063.\n",
            "epoch:44  0/1  loss_contrastive:-21.19365692138672  loss_label:0.2925840991212546.\n",
            "epoch:45  0/1  loss_contrastive:-21.099027633666992  loss_label:0.29520654198879615.\n",
            "epoch:46  0/1  loss_contrastive:-20.980010986328125  loss_label:0.29366637007581925.\n",
            "epoch:47  0/1  loss_contrastive:-20.80368995666504  loss_label:0.2905434659291425.\n",
            "epoch:48  0/1  loss_contrastive:-20.62816047668457  loss_label:0.29193283830371525.\n",
            "epoch:49  0/1  loss_contrastive:-20.52851104736328  loss_label:0.2911138886478388.\n",
            "epoch:49  1/1  loss_contrastive:-20.52851104736328  loss_label:0.2911138886478388.\n",
            "\n",
            "Epoch 49, 1/1.\n",
            "Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.7193, 0.7974, 0.9987, 0.8016, 0.8868, 0.5421)\n",
            "Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.7193, 0.7974, 0.9987, 0.8016, 0.8868, 0.5421)\n",
            "epoch:50  0/1  loss_contrastive:-20.52401351928711  loss_label:0.2900118085525461.\n",
            "epoch:51  0/1  loss_contrastive:-20.466899871826172  loss_label:0.2898676008579767.\n",
            "epoch:52  0/1  loss_contrastive:-20.533485412597656  loss_label:0.28923733991869355.\n",
            "epoch:53  0/1  loss_contrastive:-20.645721435546875  loss_label:0.2886675683473967.\n",
            "epoch:54  0/1  loss_contrastive:-20.708494186401367  loss_label:0.2903442574794493.\n",
            "epoch:55  0/1  loss_contrastive:-20.782548904418945  loss_label:0.28807073056355054.\n",
            "epoch:56  0/1  loss_contrastive:-20.82979393005371  loss_label:0.2879944379536985.\n",
            "epoch:57  0/1  loss_contrastive:-20.943567276000977  loss_label:0.28707584520682145.\n",
            "epoch:58  0/1  loss_contrastive:-21.063838958740234  loss_label:0.2872931972075459.\n",
            "epoch:59  0/1  loss_contrastive:-21.127384185791016  loss_label:0.2882077366373911.\n",
            "epoch:60  0/1  loss_contrastive:-21.231298446655273  loss_label:0.28520045386082654.\n",
            "epoch:61  0/1  loss_contrastive:-21.303133010864258  loss_label:0.28685888985849034.\n",
            "epoch:62  0/1  loss_contrastive:-21.342926025390625  loss_label:0.28502323932904594.\n",
            "epoch:63  0/1  loss_contrastive:-21.33652114868164  loss_label:0.2853954752116415.\n",
            "epoch:64  0/1  loss_contrastive:-21.466796875  loss_label:0.28470156718600864.\n",
            "epoch:65  0/1  loss_contrastive:-21.508501052856445  loss_label:0.2850775684366334.\n",
            "epoch:66  0/1  loss_contrastive:-21.511323928833008  loss_label:0.28501496780445224.\n",
            "epoch:67  0/1  loss_contrastive:-21.638900756835938  loss_label:0.28507978673132167.\n",
            "epoch:68  0/1  loss_contrastive:-21.6709041595459  loss_label:0.28287894937248165.\n",
            "epoch:69  0/1  loss_contrastive:-21.75060272216797  loss_label:0.28182515154066656.\n",
            "epoch:70  0/1  loss_contrastive:-21.81012535095215  loss_label:0.2819190743527995.\n",
            "epoch:71  0/1  loss_contrastive:-21.795364379882812  loss_label:0.28491276716458946.\n",
            "epoch:72  0/1  loss_contrastive:-21.949573516845703  loss_label:0.29769871636442513.\n",
            "epoch:73  0/1  loss_contrastive:-21.57752227783203  loss_label:0.3348056208963208.\n",
            "epoch:74  0/1  loss_contrastive:-22.043821334838867  loss_label:0.28555169142571896.\n",
            "epoch:75  0/1  loss_contrastive:-22.20935821533203  loss_label:0.297074852952228.\n",
            "epoch:76  0/1  loss_contrastive:-22.086833953857422  loss_label:0.291763122826488.\n",
            "epoch:77  0/1  loss_contrastive:-21.819488525390625  loss_label:0.28650900829105314.\n",
            "epoch:78  0/1  loss_contrastive:-21.431800842285156  loss_label:0.28879439375283356.\n",
            "epoch:79  0/1  loss_contrastive:-21.084203720092773  loss_label:0.2931040723307463.\n",
            "epoch:80  0/1  loss_contrastive:-20.91526985168457  loss_label:0.2927082535449191.\n",
            "epoch:81  0/1  loss_contrastive:-20.889408111572266  loss_label:0.2902677915150982.\n",
            "epoch:82  0/1  loss_contrastive:-20.89070701599121  loss_label:0.28922222042290935.\n",
            "epoch:83  0/1  loss_contrastive:-20.885822296142578  loss_label:0.2909618062315432.\n",
            "epoch:84  0/1  loss_contrastive:-20.812061309814453  loss_label:0.29162057621326526.\n",
            "epoch:85  0/1  loss_contrastive:-20.66754913330078  loss_label:0.28968290541166775.\n",
            "epoch:86  0/1  loss_contrastive:-20.453786849975586  loss_label:0.2880907406452529.\n",
            "epoch:87  0/1  loss_contrastive:-20.165699005126953  loss_label:0.28722944251441007.\n",
            "epoch:88  0/1  loss_contrastive:-19.920879364013672  loss_label:0.2881116289250984.\n",
            "epoch:89  0/1  loss_contrastive:-19.78293228149414  loss_label:0.28418296553941946.\n",
            "epoch:90  0/1  loss_contrastive:-19.710853576660156  loss_label:0.28362758789551157.\n",
            "epoch:91  0/1  loss_contrastive:-19.55606460571289  loss_label:0.2846791374461777.\n",
            "epoch:92  0/1  loss_contrastive:-19.343639373779297  loss_label:0.28324897078808664.\n",
            "epoch:93  0/1  loss_contrastive:-19.148635864257812  loss_label:0.2819445175435664.\n",
            "epoch:94  0/1  loss_contrastive:-18.962255477905273  loss_label:0.28208975617865495.\n",
            "epoch:95  0/1  loss_contrastive:-18.851537704467773  loss_label:0.2817001395317191.\n",
            "epoch:96  0/1  loss_contrastive:-18.78999137878418  loss_label:0.2797314886497441.\n",
            "epoch:97  0/1  loss_contrastive:-18.80097770690918  loss_label:0.2793658402723505.\n",
            "epoch:98  0/1  loss_contrastive:-18.814619064331055  loss_label:0.27959971644187326.\n",
            "epoch:99  0/1  loss_contrastive:-18.871002197265625  loss_label:0.2787619055480155.\n",
            "epoch:99  1/1  loss_contrastive:-18.871002197265625  loss_label:0.2787619055480155.\n",
            "\n",
            "Epoch 99, 1/1.\n",
            "Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.7697, 0.7979, 0.9984, 0.8020, 0.8870, 0.5446)\n",
            "Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      (0.7697, 0.7979, 0.9984, 0.8020, 0.8870, 0.5446)\n",
            "epoch:100  0/1  loss_contrastive:-19.076324462890625  loss_label:0.28199615321663735.\n",
            "repeat: 0\n",
            "mean auc, micro_f1, binary_f1, macro_f1:[0.76969316 0.80204388 0.88700353 0.5445917 ]\n"
          ]
        }
      ],
      "source": [
        "############################################### Training ############################################################\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for m in range(1):\n",
        "    g = graph_user.edge_type_subgraph(args.edge_type)\n",
        "    pos_weight = None\n",
        "    model = Model(args).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    dataset = NodeBatch(np.arange(g.num_nodes()))\n",
        "    dataloader_nodes = torch.utils.data.DataLoader(dataset, batch_size=args.loss_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    dataset = LabelPairs(label_train)\n",
        "    dataloader_labels = torch.utils.data.DataLoader(dataset, batch_size=int(args.loss_batch_size*len(label_train)/g.num_nodes()), shuffle=True)\n",
        "    res = defaultdict(list)\n",
        "\n",
        "    for e in range(args.pretrain_epochs+args.finetune_epochs):\n",
        "        g_attr, g_stru = GraphAug(g, args)\n",
        "        cnt = 0\n",
        "\n",
        "        for nids, (pair, y) in zip(dataloader_nodes, dataloader_labels):\n",
        "            u, v = pair.T\n",
        "            y = y.to(device)\n",
        "            nids = torch.unique(torch.cat((u, v), dim=-1))\n",
        "\n",
        "            \n",
        "            embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg = model(g_attr, g_stru, nids, device)\n",
        "            loss_contrastive = model.compute_contrastive_loss(device, embs_attr_pos[nids], embs_stru_pos[nids], embs_attr_neg[nids], embs_stru_neg[nids])\n",
        "\n",
        "            y_score = model.predict_combine((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), u, v, device)\n",
        "            loss_label = model.compute_label_loss(y_score, y, pos_weight, device)\n",
        "\n",
        "            loss = args.alpha  * loss_contrastive + loss_label\n",
        "\n",
        "            print(f'epoch:{e}  {cnt}/{len(dataloader_nodes)}  loss_contrastive:{loss_contrastive}  loss_label:{loss_label}.')\n",
        "\n",
        "            opt.zero_grad() \n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            cnt += 1\n",
        "\n",
        "            \n",
        "            \n",
        "            if (e+1) % 50 ==0:\n",
        "                print(f'epoch:{e}  {cnt}/{len(dataloader_nodes)}  loss_contrastive:{loss_contrastive}  loss_label:{loss_label}.')\n",
        "                with torch.no_grad():\n",
        "                    embs, (attn_attr_pos, attn_stru_pos, attn_attr_neg, attn_stru_neg) = model.inference(g, g, label_ids, device)\n",
        "                    train_auc, train_prec, train_recl, train_micro_f1, train_binary_f1, train_macro_f1 = eval_metric(embs, model, label_train, args, device)\n",
        "                    test_auc, test_prec, test_recl, test_micro_f1, test_binary_f1, test_macro_f1 = eval_metric(embs, model, label_test, args, device)\n",
        "\n",
        "                    res['train'].append([train_auc, train_prec, train_recl, train_micro_f1, train_binary_f1, train_macro_f1])\n",
        "                    res['test'].append([test_auc, test_prec, test_recl, test_micro_f1, test_binary_f1, test_macro_f1])\n",
        "                    print(f'Epoch {e}, {cnt}/{len(dataloader_nodes)}.')\n",
        "                    print(f'Training (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      ({train_auc:.4f}, {train_prec:.4f}, {train_recl:.4f}, {train_micro_f1:.4f}, {train_binary_f1:.4f}, {train_macro_f1:.4f})')\n",
        "                    print(f'Testing  (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):      ({test_auc:.4f}, {test_prec:.4f}, {test_recl:.4f}, {test_micro_f1:.4f}, {test_binary_f1:.4f}, {test_macro_f1:.4f})')\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'repeat: {m}')\n",
        "    test_results.append([test_auc, test_micro_f1, test_binary_f1, test_macro_f1])\n",
        "    \n",
        "print(f'mean auc, micro_f1, binary_f1, macro_f1:{np.array(test_results).sum(0)/np.array(test_results).shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Results\n",
        "\n",
        "> After 100 epochs on SGCL\n",
        "\n",
        "*   (AUC, Precision, Recall, Micro_F1, Binary_F1, Macro_F1):\n",
        "* (0.7697, 0.7979, 0.9984, 0.8020, 0.8870, 0.5446)\n",
        "\n",
        "* loss_contrastive:-18.871002197265625  loss_label:0.2787619055480155.\n",
        "\n",
        "\n",
        "* mean auc, micro_f1, binary_f1, macro_f1:[0.76969316 0.80204388 0.88700353 0.5445917 ]\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_fu0_MFWBGBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For training on Logistic Regression, SVM and RandomForest"
      ],
      "metadata": {
        "id": "UiPoSvOkBYLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = graph_user.edge_type_subgraph(args.edge_type)\n",
        "e1,e2 = GraphAug(g, args)"
      ],
      "metadata": {
        "id": "brXCAY6HO-tH"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "JoxIKAoLg40T"
      },
      "outputs": [],
      "source": [
        "model = Model(args).to(device)\n",
        "embs_attr_pos, embs_stru_pos, embs_attr_neg, embs_stru_neg = model(e1, e2, nids, device)\n",
        "embs = torch.cat((embs_attr_pos,embs_stru_pos,embs_attr_neg, embs_stru_neg), dim=-1)\n",
        "te=model.transform(embs.cuda())\n",
        "norm_te=F.normalize(te, p=2, dim=1)\n",
        "t_np = norm_te.cpu().data.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "k9EgffCH2g1e"
      },
      "outputs": [],
      "source": [
        "df2['n1']=df2['src']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "uMA16Fqy8v7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591cc52f-bfcb-429d-c076-11ad495817d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185627/185627 [00:24<00:00, 7600.71it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(df2.index):\n",
        "  # print(i)\n",
        "  # print(t_np[df2['src'].loc[i]])\n",
        "  df2['n1'].loc[i]=[t_np[df2['src'].loc[i]]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2['n2']=df2['dst']"
      ],
      "metadata": {
        "id": "ybgmHmFdEZEO"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "xA52K0fF2gtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e290ede4-47a5-4454-f59f-0ac453f9752d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185627/185627 [00:23<00:00, 7933.24it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(df2.index):\n",
        "  # print(i)\n",
        "  # print(t_np[df2['src'].loc[i]])\n",
        "  df2['n2'].loc[i]=[t_np[df2['dst'].loc[i]]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ih9l5YjRN6RA"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HADAMARD"
      ],
      "metadata": {
        "id": "kziA11hAN_mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "had_emb= []\n",
        "had_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=np.multiply(np.asarray(df2['n1'][i]), np.asarray(df2['n2'][i]))\n",
        "  had_emb.append(prod[0])\n",
        "  had_label.append(df2['label'].loc[i])\n",
        "  # print(prod)\n",
        "  # df_hadamard['had'].loc[i] = [prod]"
      ],
      "metadata": {
        "id": "_QR4oppjIuK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b087833-13cb-4e77-def2-0876417cc8f0"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185627/185627 [00:08<00:00, 23188.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# had_emb"
      ],
      "metadata": {
        "id": "YZ3no2T_9iDu"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(had_emb)==len(had_label)"
      ],
      "metadata": {
        "id": "0932ypRP6yJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b012ca-5b11-4e39-d246-9fb76871f8be"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':had_emb, 'label':had_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"HADAMARD:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ],
      "metadata": {
        "id": "reRGo3bT-JDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85729c60-4a90-459f-e90d-a31aeb87a10b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADAMARD:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.7798577816085762\n",
            "F1 macro: 0.43851853426236365\n",
            "F1 score: 0.8763034825117673\n",
            "AUC score: 0.5001662156779296\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.7906049668695793\n",
            "F1 macro: 0.5004797526240701\n",
            "F1 score: 0.8811678385814735\n",
            "AUC score: 0.5301441083040552\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.8012982815277704\n",
            "F1 macro: 0.5793737391461902\n",
            "F1 score: 0.8849016273227966\n",
            "AUC score: 0.5748350101621669\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenated"
      ],
      "metadata": {
        "id": "TR1erZ_5OGHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_emb= []\n",
        "conc_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=prod=np.concatenate((df2['n1'][i],df2['n2'][i]))\n",
        "  conc_emb.append(prod[0])\n",
        "  conc_label.append(df2['label'].loc[i])\n",
        "  # print(prod)\n",
        "  # df_hadamard['had'].loc[i] = [prod]"
      ],
      "metadata": {
        "id": "62rs-c5fkGhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff377077-7e66-457b-ed48-16a0e0b1604c"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185627/185627 [00:07<00:00, 25197.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':conc_emb, 'label':conc_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "\n",
        "print(\"Concatenated:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ],
      "metadata": {
        "id": "CTRb3qQKl6Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c38d16-84ce-4c8b-e46e-c76e1fc3471e"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated:\n",
            "Logistic Regression:\n",
            "F1 micro: 0.7806119700479448\n",
            "F1 macro: 0.4421147604728336\n",
            "F1 score: 0.8766749943220532\n",
            "AUC score: 0.5018787539042293\n",
            "\n",
            "SVM:\n",
            "F1 micro: 0.7811506760760653\n",
            "F1 macro: 0.44514163014017866\n",
            "F1 score: 0.8769256403653604\n",
            "AUC score: 0.5032775615700767\n",
            "\n",
            "Random Forest:\n",
            "F1 micro: 0.78904271938803\n",
            "F1 macro: 0.5920150377576325\n",
            "F1 score: 0.8755363442774052\n",
            "AUC score: 0.5826022858194332\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1 Norm\n"
      ],
      "metadata": {
        "id": "w2nDBDvMOeFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_emb= []\n",
        "l1_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=abs(np.array(df2['n1'][i])-np.array(df2['n2'][i]))\n",
        "  l1_emb.append(prod[0])\n",
        "  l1_label.append(df2['label'].loc[i])"
      ],
      "metadata": {
        "id": "qDjvJBXMkzot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a773f7-8bf7-472d-d796-38e26399b248"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185627/185627 [00:06<00:00, 27140.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':l1_emb, 'label':l1_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"L1 Norm:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ],
      "metadata": {
        "id": "uJYy816wmEK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 Norm"
      ],
      "metadata": {
        "id": "gAGbZ0oNOjo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l2_emb= []\n",
        "l2_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=abs(np.array(df2['n1'][i])-np.array(df2['n2'][i]))**2\n",
        "  l2_emb.append(prod[0])\n",
        "  l2_label.append(df2['label'].loc[i])"
      ],
      "metadata": {
        "id": "hDyZplt7lLPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':l2_emb, 'label':l2_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"L2 Norm:\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ],
      "metadata": {
        "id": "RUXUn5BUmLLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average"
      ],
      "metadata": {
        "id": "n9LuU0yiOpgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_emb= []\n",
        "avg_label = [] \n",
        "for i in tqdm(df2.index):\n",
        "  # print(df_hadamard['n1'][i])\n",
        "  prod=(np.array(df2['n1'][i])+np.array(df2['n2'][i]))/2\n",
        "  avg_emb.append(prod[0])\n",
        "  avg_label.append(df2['label'].loc[i])"
      ],
      "metadata": {
        "id": "AVTy8cK7lLIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# load data into a pandas DataFrame\n",
        "# data = pd.read_csv('data.csv')\n",
        "data=pd.DataFrame({'features':avg_emb, 'label':avg_label})\n",
        "\n",
        "# split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# separate features and labels\n",
        "X_train = train_data['features'].values.tolist()\n",
        "y_train = train_data['label'].values.tolist()\n",
        "\n",
        "X_test = test_data['features'].values.tolist()\n",
        "y_test = test_data['label'].values.tolist()\n",
        "\n",
        "# train and evaluate logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "f1_micro_lr = f1_score(y_test, y_pred_lr, average='micro')\n",
        "f1_macro_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "\n",
        "# train and evaluate SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "f1_micro_svm = f1_score(y_test, y_pred_svm, average='micro')\n",
        "f1_macro_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "\n",
        "# train and evaluate random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "f1_micro_rf = f1_score(y_test, y_pred_rf, average='micro')\n",
        "f1_macro_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
        "\n",
        "# print results\n",
        "print(\"Average :\")\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"F1 micro: {f1_micro_lr}\")\n",
        "print(f\"F1 macro: {f1_macro_lr}\")\n",
        "print(f\"F1 score: {f1_lr}\")\n",
        "print(f\"AUC score: {auc_lr}\\n\")\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(f\"F1 micro: {f1_micro_svm}\")\n",
        "print(f\"F1 macro: {f1_macro_svm}\")\n",
        "print(f\"F1 score: {f1_svm}\")\n",
        "print(f\"AUC score: {auc_svm}\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"F1 micro: {f1_micro_rf}\")\n",
        "print(f\"F1 macro: {f1_macro_rf}\")\n",
        "print(f\"F1 score: {f1_rf}\")\n",
        "print(f\"AUC score: {auc_rf}\\n\")\n"
      ],
      "metadata": {
        "id": "zhksY7rJkFt1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}